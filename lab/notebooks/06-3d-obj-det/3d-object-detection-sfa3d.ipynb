{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2255937,"sourceType":"datasetVersion","datasetId":1357458}],"dockerImageVersionId":30146,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Super Fast and Accurate 3D Object Detection\nPredicting Cars in Lidar data using SFA3D\n\nGithub Link to Original Code: https://github.com/maudzung/SFA3D","metadata":{}},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"markdown","source":"## KITT Data Configuration","metadata":{}},{"cell_type":"code","source":"import math\n\nimport numpy as np\n\n# Car and Van ==> Car class\n# Pedestrian and Person_Sitting ==> Pedestrian Class\nCLASS_NAME_TO_ID = {\n    'Pedestrian': 0,\n    'Car': 1,\n    'Cyclist': 2,\n    'Van': 1,\n    'Truck': -3,\n    'Person_sitting': 0,\n    'Tram': -99,\n    'Misc': -99,\n    'DontCare': -1\n}\n\ncolors = [[0, 255, 255], [0, 0, 255], [255, 0, 0], [255, 120, 0],\n          [255, 120, 120], [0, 120, 0], [120, 255, 255], [120, 0, 255]]\n\n#####################################################################################\nboundary = {\n    \"minX\": 0,\n    \"maxX\": 50,\n    \"minY\": -25,\n    \"maxY\": 25,\n    \"minZ\": -2.73,\n    \"maxZ\": 1.27\n}\n\nbound_size_x = boundary['maxX'] - boundary['minX']\nbound_size_y = boundary['maxY'] - boundary['minY']\nbound_size_z = boundary['maxZ'] - boundary['minZ']\n\nboundary_back = {\n    \"minX\": -50,\n    \"maxX\": 0,\n    \"minY\": -25,\n    \"maxY\": 25,\n    \"minZ\": -2.73,\n    \"maxZ\": 1.27\n}\n\nBEV_WIDTH = 608  # across y axis -25m ~ 25m\nBEV_HEIGHT = 608  # across x axis 0m ~ 50m\nDISCRETIZATION = (boundary[\"maxX\"] - boundary[\"minX\"]) / BEV_HEIGHT\n\n# maximum number of points per voxel\nT = 35\n\n# voxel size\nvd = 0.1  # z\nvh = 0.05  # y\nvw = 0.05  # x\n\n# voxel grid\nW = math.ceil(bound_size_x / vw)\nH = math.ceil(bound_size_y / vh)\nD = math.ceil(bound_size_z / vd)\n\n# Following parameters are calculated as an average from KITTI dataset for simplicity\n#####################################################################################\nTr_velo_to_cam = np.array([\n    [7.49916597e-03, -9.99971248e-01, -8.65110297e-04, -6.71807577e-03],\n    [1.18652889e-02, 9.54520517e-04, -9.99910318e-01, -7.33152811e-02],\n    [9.99882833e-01, 7.49141178e-03, 1.18719929e-02, -2.78557062e-01],\n    [0, 0, 0, 1]\n])\n\n# cal mean from train set\nR0 = np.array([\n    [0.99992475, 0.00975976, -0.00734152, 0],\n    [-0.0097913, 0.99994262, -0.00430371, 0],\n    [0.00729911, 0.0043753, 0.99996319, 0],\n    [0, 0, 0, 1]\n])\n\nP2 = np.array([[719.787081, 0., 608.463003, 44.9538775],\n               [0., 719.787081, 174.545111, 0.1066855],\n               [0., 0., 1., 3.0106472e-03],\n               [0., 0., 0., 0]\n               ])\n\nR0_inv = np.linalg.inv(R0)\nTr_velo_to_cam_inv = np.linalg.inv(Tr_velo_to_cam)\nP2_inv = np.linalg.pinv(P2)\n#####################################################################################","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:44:51.658012Z","iopub.execute_input":"2021-12-13T06:44:51.658285Z","iopub.status.idle":"2021-12-13T06:44:51.677196Z","shell.execute_reply.started":"2021-12-13T06:44:51.658245Z","shell.execute_reply":"2021-12-13T06:44:51.676194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Configurations","metadata":{}},{"cell_type":"code","source":"import os\n\nimport torch\nfrom easydict import EasyDict as edict\n\ndef parse_configs():\n    config_dict = {}\n    config_dict['seed'] = 2020\n    config_dict['saved_fn'] = 'fpn_resnet_18'\n    config_dict['root_dir'] = './'\n    \n    ####################################################################\n    ##############     Model configs            ########################\n    ####################################################################\n    config_dict['arch'] = 'fpn_resnet_18'\n    config_dict['pretrained_path'] = None\n    ####################################################################\n    ##############     Dataloader and Running configs            #######\n    ####################################################################\n    config_dict['hflip_prob'] = 0.5\n    config_dict['no_val'] = False\n    config_dict['num_samples'] = None\n    config_dict['num_workers'] = 4\n    config_dict['batch_size'] = 16\n    config_dict['print_freq'] = 50\n    config_dict['tensorboard_freq'] = 50\n    config_dict['checkpoint_freq'] = 2\n    ####################################################################\n    ##############     Training strategy            ####################\n    ####################################################################\n    config_dict['start_epoch'] = 1\n    config_dict['num_epochs'] = 300\n    config_dict['lr_type'] = 'cosin'\n    config_dict['lr'] = 0.001\n    config_dict['minimum_lr'] = 1e-7\n    config_dict['momentum'] = 0.949\n    config_dict['weight_decay'] = 0\n    config_dict['optimizer_type'] = 'adam'\n    config_dict['steps'] = [150, 180]\n\n    ####################################################################\n    ##############     Loss weight            ##########################\n    ####################################################################\n\n    ####################################################################\n    ##############     Distributed Data Parallel            ############\n    ####################################################################\n    config_dict['world_size'] = -1\n    config_dict['rank'] = -1\n    config_dict['dist_url'] = 'tcp://127.0.0.1:29500'\n    config_dict['gpu_idx'] = 0\n    config_dict['no_cuda'] = False\n    config_dict['multiprocessing_distributed'] = False\n    ####################################################################\n    ##############     Evaluation configurations     ###################\n    ####################################################################\n    config_dict['evaluate'] = False\n    config_dict['resume_path'] = None\n    config_dict['K'] = 50\n\n    configs = edict(config_dict)\n\n    ####################################################################\n    ############## Hardware configurations #############################\n    ####################################################################\n    configs.device = torch.device('cpu' if configs.no_cuda else 'cuda')\n    configs.ngpus_per_node = torch.cuda.device_count()\n\n    configs.pin_memory = True\n    configs.input_size = (608, 608)\n    configs.hm_size = (152, 152)\n    configs.down_ratio = 4\n    configs.max_objects = 50\n\n    configs.imagenet_pretrained = True\n    configs.head_conv = 64\n    configs.num_classes = 3\n    configs.num_center_offset = 2\n    configs.num_z = 1\n    configs.num_dim = 3\n    configs.num_direction = 2  # sin, cos\n\n    configs.heads = {\n        'hm_cen': configs.num_classes,\n        'cen_offset': configs.num_center_offset,\n        'direction': configs.num_direction,\n        'z_coor': configs.num_z,\n        'dim': configs.num_dim\n    }\n\n    configs.num_input_features = 4\n\n    ####################################################################\n    ############## Dataset, logs, Checkpoints dir ######################\n    ####################################################################\n    configs.dataset_dir = '../input/kitti-3d-object-detection-dataset/'\n    configs.checkpoints_dir = os.path.join(configs.root_dir, 'checkpoints', configs.saved_fn)\n    configs.logs_dir = os.path.join(configs.root_dir, 'logs', configs.saved_fn)\n\n    if not os.path.isdir(configs.checkpoints_dir):\n        os.makedirs(configs.checkpoints_dir)\n    if not os.path.isdir(configs.logs_dir):\n        os.makedirs(configs.logs_dir)\n\n    return configs","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:44:51.679985Z","iopub.execute_input":"2021-12-13T06:44:51.680539Z","iopub.status.idle":"2021-12-13T06:44:51.71197Z","shell.execute_reply.started":"2021-12-13T06:44:51.680494Z","shell.execute_reply":"2021-12-13T06:44:51.711037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Processing\n","metadata":{}},{"cell_type":"markdown","source":"## Transformation Functions","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport math\n\nimport numpy as np\nimport torch\n\ndef angle_in_limit(angle):\n    # To limit the angle in -pi/2 - pi/2\n    limit_degree = 5\n    while angle >= np.pi / 2:\n        angle -= np.pi\n    while angle < -np.pi / 2:\n        angle += np.pi\n    if abs(angle + np.pi / 2) < limit_degree / 180 * np.pi:\n        angle = np.pi / 2\n    return angle\n\n\ndef camera_to_lidar(x, y, z, V2C=None, R0=None, P2=None):\n    p = np.array([x, y, z, 1])\n    if V2C is None or R0 is None:\n        p = np.matmul(R0_inv, p)\n        p = np.matmul(Tr_velo_to_cam_inv, p)\n    else:\n        R0_i = np.zeros((4, 4))\n        R0_i[:3, :3] = R0\n        R0_i[3, 3] = 1\n        p = np.matmul(np.linalg.inv(R0_i), p)\n        p = np.matmul(inverse_rigid_trans(V2C), p)\n    p = p[0:3]\n    return tuple(p)\n\n\ndef lidar_to_camera(x, y, z, V2C=None, R0=None, P2=None):\n    p = np.array([x, y, z, 1])\n    if V2C is None or R0 is None:\n        p = np.matmul(Tr_velo_to_cam, p)\n        p = np.matmul(R0, p)\n    else:\n        p = np.matmul(V2C, p)\n        p = np.matmul(R0, p)\n    p = p[0:3]\n    return tuple(p)\n\n\ndef camera_to_lidar_point(points):\n    # (N, 3) -> (N, 3)\n    N = points.shape[0]\n    points = np.hstack([points, np.ones((N, 1))]).T  # (N,4) -> (4,N)\n\n    points = np.matmul(R0_inv, points)\n    points = np.matmul(Tr_velo_to_cam_inv, points).T  # (4, N) -> (N, 4)\n    points = points[:, 0:3]\n    return points.reshape(-1, 3)\n\n\ndef lidar_to_camera_point(points, V2C = None):\n    # (N, 3) -> (N, 3)\n    N = points.shape[0]\n    points = np.hstack([points, np.ones((N, 1))]).T\n\n    if V2C is None or R0 is None:\n        points = np.matmul(Tr_velo_to_cam, points)\n        points = np.matmul(R0, points).T\n    else:\n        points = np.matmul(V2C, points)\n        points = np.matmul(R0, points).T\n    points = points[:, 0:3]\n    return points.reshape(-1, 3)\n\n\ndef camera_to_lidar_box(boxes, V2C=None, R0=None, P2=None):\n    # (N, 7) -> (N, 7) x,y,z,h,w,l,r\n    ret = []\n    for box in boxes:\n        x, y, z, h, w, l, ry = box\n        (x, y, z), h, w, l, rz = camera_to_lidar(x, y, z, V2C=V2C, R0=R0, P2=P2), h, w, l, -ry - np.pi / 2\n        # rz = angle_in_limit(rz)\n        ret.append([x, y, z, h, w, l, rz])\n    return np.array(ret).reshape(-1, 7)\n\n\ndef lidar_to_camera_box(boxes, V2C=None, R0=None, P2=None):\n    # (N, 7) -> (N, 7) x,y,z,h,w,l,r\n    ret = []\n    for box in boxes:\n        x, y, z, h, w, l, rz = box\n        (x, y, z), h, w, l, ry = lidar_to_camera(x, y, z, V2C=V2C, R0=R0, P2=P2), h, w, l, -rz - np.pi / 2\n        # ry = angle_in_limit(ry)\n        ret.append([x, y, z, h, w, l, ry])\n    return np.array(ret).reshape(-1, 7)\n\n\ndef center_to_corner_box2d(boxes_center, coordinate='lidar'):\n    # (N, 5) -> (N, 4, 2)\n    N = boxes_center.shape[0]\n    boxes3d_center = np.zeros((N, 7))\n    boxes3d_center[:, [0, 1, 4, 5, 6]] = boxes_center\n    boxes3d_corner = center_to_corner_box3d(boxes3d_center, coordinate=coordinate)\n\n    return boxes3d_corner[:, 0:4, 0:2]\n\n\ndef center_to_corner_box3d(boxes_center, coordinate='lidar'):\n    # (N, 7) -> (N, 8, 3)\n    N = boxes_center.shape[0]\n    ret = np.zeros((N, 8, 3), dtype=np.float32)\n\n    if coordinate == 'camera':\n        boxes_center = camera_to_lidar_box(boxes_center)\n\n    for i in range(N):\n        box = boxes_center[i]\n        translation = box[0:3]\n        size = box[3:6]\n        rotation = [0, 0, box[-1]]\n\n        h, w, l = size[0], size[1], size[2]\n        trackletBox = np.array([  # in velodyne coordinates around zero point and without orientation yet\n            [-l / 2, -l / 2, l / 2, l / 2, -l / 2, -l / 2, l / 2, l / 2], \\\n            [w / 2, -w / 2, -w / 2, w / 2, w / 2, -w / 2, -w / 2, w / 2], \\\n            [0, 0, 0, 0, h, h, h, h]])\n\n        # re-create 3D bounding box in velodyne coordinate system\n        yaw = rotation[2]\n        rotMat = np.array([\n            [np.cos(yaw), -np.sin(yaw), 0.0],\n            [np.sin(yaw), np.cos(yaw), 0.0],\n            [0.0, 0.0, 1.0]])\n        cornerPosInVelo = np.dot(rotMat, trackletBox) + np.tile(translation, (8, 1)).T\n        box3d = cornerPosInVelo.transpose()\n        ret[i] = box3d\n\n    if coordinate == 'camera':\n        for idx in range(len(ret)):\n            ret[idx] = lidar_to_camera_point(ret[idx])\n\n    return ret\n\n\nCORNER2CENTER_AVG = True\n\n\ndef corner_to_center_box3d(boxes_corner, coordinate='camera'):\n    # (N, 8, 3) -> (N, 7) x,y,z,h,w,l,ry/z\n    if coordinate == 'lidar':\n        for idx in range(len(boxes_corner)):\n            boxes_corner[idx] = lidar_to_camera_point(boxes_corner[idx])\n\n    ret = []\n    for roi in boxes_corner:\n        if CORNER2CENTER_AVG:  # average version\n            roi = np.array(roi)\n            h = abs(np.sum(roi[:4, 1] - roi[4:, 1]) / 4)\n            w = np.sum(\n                np.sqrt(np.sum((roi[0, [0, 2]] - roi[3, [0, 2]]) ** 2)) +\n                np.sqrt(np.sum((roi[1, [0, 2]] - roi[2, [0, 2]]) ** 2)) +\n                np.sqrt(np.sum((roi[4, [0, 2]] - roi[7, [0, 2]]) ** 2)) +\n                np.sqrt(np.sum((roi[5, [0, 2]] - roi[6, [0, 2]]) ** 2))\n            ) / 4\n            l = np.sum(\n                np.sqrt(np.sum((roi[0, [0, 2]] - roi[1, [0, 2]]) ** 2)) +\n                np.sqrt(np.sum((roi[2, [0, 2]] - roi[3, [0, 2]]) ** 2)) +\n                np.sqrt(np.sum((roi[4, [0, 2]] - roi[5, [0, 2]]) ** 2)) +\n                np.sqrt(np.sum((roi[6, [0, 2]] - roi[7, [0, 2]]) ** 2))\n            ) / 4\n            x = np.sum(roi[:, 0], axis=0) / 8\n            y = np.sum(roi[0:4, 1], axis=0) / 4\n            z = np.sum(roi[:, 2], axis=0) / 8\n            ry = np.sum(\n                math.atan2(roi[2, 0] - roi[1, 0], roi[2, 2] - roi[1, 2]) +\n                math.atan2(roi[6, 0] - roi[5, 0], roi[6, 2] - roi[5, 2]) +\n                math.atan2(roi[3, 0] - roi[0, 0], roi[3, 2] - roi[0, 2]) +\n                math.atan2(roi[7, 0] - roi[4, 0], roi[7, 2] - roi[4, 2]) +\n                math.atan2(roi[0, 2] - roi[1, 2], roi[1, 0] - roi[0, 0]) +\n                math.atan2(roi[4, 2] - roi[5, 2], roi[5, 0] - roi[4, 0]) +\n                math.atan2(roi[3, 2] - roi[2, 2], roi[2, 0] - roi[3, 0]) +\n                math.atan2(roi[7, 2] - roi[6, 2], roi[6, 0] - roi[7, 0])\n            ) / 8\n            if w > l:\n                w, l = l, w\n                ry = ry - np.pi / 2\n            elif l > w:\n                l, w = w, l\n                ry = ry - np.pi / 2\n            ret.append([x, y, z, h, w, l, ry])\n\n        else:  # max version\n            h = max(abs(roi[:4, 1] - roi[4:, 1]))\n            w = np.max(\n                np.sqrt(np.sum((roi[0, [0, 2]] - roi[3, [0, 2]]) ** 2)) +\n                np.sqrt(np.sum((roi[1, [0, 2]] - roi[2, [0, 2]]) ** 2)) +\n                np.sqrt(np.sum((roi[4, [0, 2]] - roi[7, [0, 2]]) ** 2)) +\n                np.sqrt(np.sum((roi[5, [0, 2]] - roi[6, [0, 2]]) ** 2))\n            )\n            l = np.max(\n                np.sqrt(np.sum((roi[0, [0, 2]] - roi[1, [0, 2]]) ** 2)) +\n                np.sqrt(np.sum((roi[2, [0, 2]] - roi[3, [0, 2]]) ** 2)) +\n                np.sqrt(np.sum((roi[4, [0, 2]] - roi[5, [0, 2]]) ** 2)) +\n                np.sqrt(np.sum((roi[6, [0, 2]] - roi[7, [0, 2]]) ** 2))\n            )\n            x = np.sum(roi[:, 0], axis=0) / 8\n            y = np.sum(roi[0:4, 1], axis=0) / 4\n            z = np.sum(roi[:, 2], axis=0) / 8\n            ry = np.sum(\n                math.atan2(roi[2, 0] - roi[1, 0], roi[2, 2] - roi[1, 2]) +\n                math.atan2(roi[6, 0] - roi[5, 0], roi[6, 2] - roi[5, 2]) +\n                math.atan2(roi[3, 0] - roi[0, 0], roi[3, 2] - roi[0, 2]) +\n                math.atan2(roi[7, 0] - roi[4, 0], roi[7, 2] - roi[4, 2]) +\n                math.atan2(roi[0, 2] - roi[1, 2], roi[1, 0] - roi[0, 0]) +\n                math.atan2(roi[4, 2] - roi[5, 2], roi[5, 0] - roi[4, 0]) +\n                math.atan2(roi[3, 2] - roi[2, 2], roi[2, 0] - roi[3, 0]) +\n                math.atan2(roi[7, 2] - roi[6, 2], roi[6, 0] - roi[7, 0])\n            ) / 8\n            if w > l:\n                w, l = l, w\n                ry = angle_in_limit(ry + np.pi / 2)\n            ret.append([x, y, z, h, w, l, ry])\n\n    if coordinate == 'lidar':\n        ret = camera_to_lidar_box(np.array(ret))\n\n    return np.array(ret)\n\n\ndef point_transform(points, tx, ty, tz, rx=0, ry=0, rz=0):\n    # Input:\n    #   points: (N, 3)\n    #   rx/y/z: in radians\n    # Output:\n    #   points: (N, 3)\n    N = points.shape[0]\n    points = np.hstack([points, np.ones((N, 1))])\n\n    mat1 = np.eye(4)\n    mat1[3, 0:3] = tx, ty, tz\n    points = np.matmul(points, mat1)\n\n    if rx != 0:\n        mat = np.zeros((4, 4))\n        mat[0, 0] = 1\n        mat[3, 3] = 1\n        mat[1, 1] = np.cos(rx)\n        mat[1, 2] = -np.sin(rx)\n        mat[2, 1] = np.sin(rx)\n        mat[2, 2] = np.cos(rx)\n        points = np.matmul(points, mat)\n\n    if ry != 0:\n        mat = np.zeros((4, 4))\n        mat[1, 1] = 1\n        mat[3, 3] = 1\n        mat[0, 0] = np.cos(ry)\n        mat[0, 2] = np.sin(ry)\n        mat[2, 0] = -np.sin(ry)\n        mat[2, 2] = np.cos(ry)\n        points = np.matmul(points, mat)\n\n    if rz != 0:\n        mat = np.zeros((4, 4))\n        mat[2, 2] = 1\n        mat[3, 3] = 1\n        mat[0, 0] = np.cos(rz)\n        mat[0, 1] = -np.sin(rz)\n        mat[1, 0] = np.sin(rz)\n        mat[1, 1] = np.cos(rz)\n        points = np.matmul(points, mat)\n\n    return points[:, 0:3]\n\n\ndef box_transform(boxes, tx, ty, tz, r=0, coordinate='lidar'):\n    # Input:\n    #   boxes: (N, 7) x y z h w l rz/y\n    # Output:\n    #   boxes: (N, 7) x y z h w l rz/y\n    boxes_corner = center_to_corner_box3d(boxes, coordinate=coordinate)  # (N, 8, 3)\n    for idx in range(len(boxes_corner)):\n        if coordinate == 'lidar':\n            boxes_corner[idx] = point_transform(boxes_corner[idx], tx, ty, tz, rz=r)\n        else:\n            boxes_corner[idx] = point_transform(boxes_corner[idx], tx, ty, tz, ry=r)\n\n    return corner_to_center_box3d(boxes_corner, coordinate=coordinate)\n\n\ndef inverse_rigid_trans(Tr):\n    ''' Inverse a rigid body transform matrix (3x4 as [R|t])\n        [R'|-R't; 0|1]\n    '''\n    inv_Tr = np.zeros_like(Tr)  # 3x4\n    inv_Tr[0:3, 0:3] = np.transpose(Tr[0:3, 0:3])\n    inv_Tr[0:3, 3] = np.dot(-np.transpose(Tr[0:3, 0:3]), Tr[0:3, 3])\n    return inv_Tr\n\n\nclass Compose(object):\n    def __init__(self, transforms, p=1.0):\n        self.transforms = transforms\n        self.p = p\n\n    def __call__(self, lidar, labels):\n        if np.random.random() <= self.p:\n            for t in self.transforms:\n                lidar, labels = t(lidar, labels)\n        return lidar, labels\n\n\nclass OneOf(object):\n    def __init__(self, transforms, p=1.0):\n        self.transforms = transforms\n        self.p = p\n\n    def __call__(self, lidar, labels):\n        if np.random.random() <= self.p:\n            choice = np.random.randint(low=0, high=len(self.transforms))\n            lidar, labels = self.transforms[choice](lidar, labels)\n\n        return lidar, labels\n\n\nclass Random_Rotation(object):\n    def __init__(self, limit_angle=np.pi / 4, p=0.5):\n        self.limit_angle = limit_angle\n        self.p = p\n\n    def __call__(self, lidar, labels):\n        \"\"\"\n        :param labels: # (N', 7) x, y, z, h, w, l, r\n        :return:\n        \"\"\"\n        if np.random.random() <= self.p:\n            angle = np.random.uniform(-self.limit_angle, self.limit_angle)\n            lidar[:, 0:3] = point_transform(lidar[:, 0:3], 0, 0, 0, rz=angle)\n            labels = box_transform(labels, 0, 0, 0, r=angle, coordinate='lidar')\n\n        return lidar, labels\n\n\nclass Random_Scaling(object):\n    def __init__(self, scaling_range=(0.95, 1.05), p=0.5):\n        self.scaling_range = scaling_range\n        self.p = p\n\n    def __call__(self, lidar, labels):\n        \"\"\"\n        :param labels: # (N', 7) x, y, z, h, w, l, r\n        :return:\n        \"\"\"\n        if np.random.random() <= self.p:\n            factor = np.random.uniform(self.scaling_range[0], self.scaling_range[0])\n            lidar[:, 0:3] = lidar[:, 0:3] * factor\n            labels[:, 0:6] = labels[:, 0:6] * factor\n\n        return lidar, labels\n\n\nclass Cutout(object):\n    \"\"\"Randomly mask out one or more patches from an image.\n    Args:\n        n_holes (int): Number of patches to cut out of each image.\n        length (int): The length (in pixels) of each square patch.\n        Refer from: https://github.com/uoguelph-mlrg/Cutout/blob/master/util/cutout.py\n    \"\"\"\n\n    def __init__(self, n_holes, ratio, fill_value=0., p=1.0):\n        self.n_holes = n_holes\n        self.ratio = ratio\n        assert 0. <= fill_value <= 1., \"the fill value is in a range of 0 to 1\"\n        self.fill_value = fill_value\n        self.p = p\n\n    def __call__(self, img, targets):\n        \"\"\"\n        Args:\n            img (Tensor): Tensor image of size (C, H, W).\n        Returns:\n            Tensor: Image with n_holes of dimension length x length cut out of it.\n        \"\"\"\n        if np.random.random() <= self.p:\n            h = img.size(1)\n            w = img.size(2)\n\n            h_cutout = int(self.ratio * h)\n            w_cutout = int(self.ratio * w)\n\n            for n in range(self.n_holes):\n                y = np.random.randint(h)\n                x = np.random.randint(w)\n\n                y1 = np.clip(y - h_cutout // 2, 0, h)\n                y2 = np.clip(y + h_cutout // 2, 0, h)\n                x1 = np.clip(x - w_cutout // 2, 0, w)\n                x2 = np.clip(x + w_cutout // 2, 0, w)\n\n                img[:, y1: y2, x1: x2] = self.fill_value  # Zero out the selected area\n                # Remove targets that are in the selected area\n                keep_target = []\n                for target_idx, target in enumerate(targets):\n                    _, _, target_x, target_y, target_w, target_l, _, _ = target\n                    if (x1 <= target_x * w <= x2) and (y1 <= target_y * h <= y2):\n                        continue\n                    keep_target.append(target_idx)\n                targets = targets[keep_target]\n\n        return img, targets","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:44:51.778854Z","iopub.execute_input":"2021-12-13T06:44:51.779367Z","iopub.status.idle":"2021-12-13T06:44:51.881868Z","shell.execute_reply.started":"2021-12-13T06:44:51.779321Z","shell.execute_reply":"2021-12-13T06:44:51.880906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Utility Functions","metadata":{}},{"cell_type":"code","source":"from __future__ import print_function\nimport os\nimport sys\n\nimport numpy as np\nimport cv2\n\n\nclass Object3d(object):\n    ''' 3d object label '''\n\n    def __init__(self, label_file_line):\n        data = label_file_line.split(' ')\n        data[1:] = [float(x) for x in data[1:]]\n        # extract label, truncation, occlusion\n        self.type = data[0]  # 'Car', 'Pedestrian', ...\n        self.cls_id = self.cls_type_to_id(self.type)\n        self.truncation = data[1]  # truncated pixel ratio [0..1]\n        self.occlusion = int(data[2])  # 0=visible, 1=partly occluded, 2=fully occluded, 3=unknown\n        self.alpha = data[3]  # object observation angle [-pi..pi]\n\n        # extract 2d bounding box in 0-based coordinates\n        self.xmin = data[4]  # left\n        self.ymin = data[5]  # top\n        self.xmax = data[6]  # right\n        self.ymax = data[7]  # bottom\n        self.box2d = np.array([self.xmin, self.ymin, self.xmax, self.ymax])\n\n        # extract 3d bounding box information\n        self.h = data[8]  # box height\n        self.w = data[9]  # box width\n        self.l = data[10]  # box length (in meters)\n        self.t = (data[11], data[12], data[13])  # location (x,y,z) in camera coord.\n        self.dis_to_cam = np.linalg.norm(self.t)\n        self.ry = data[14]  # yaw angle (around Y-axis in camera coordinates) [-pi..pi]\n        self.score = data[15] if data.__len__() == 16 else -1.0\n        self.level_str = None\n        self.level = self.get_obj_level()\n\n    def cls_type_to_id(self, cls_type):\n        if cls_type not in CLASS_NAME_TO_ID.keys():\n            return -1\n\n        return CLASS_NAME_TO_ID[cls_type]\n\n    def get_obj_level(self):\n        height = float(self.box2d[3]) - float(self.box2d[1]) + 1\n\n        if height >= 40 and self.truncation <= 0.15 and self.occlusion <= 0:\n            self.level_str = 'Easy'\n            return 1  # Easy\n        elif height >= 25 and self.truncation <= 0.3 and self.occlusion <= 1:\n            self.level_str = 'Moderate'\n            return 2  # Moderate\n        elif height >= 25 and self.truncation <= 0.5 and self.occlusion <= 2:\n            self.level_str = 'Hard'\n            return 3  # Hard\n        else:\n            self.level_str = 'UnKnown'\n            return 4\n\n    def print_object(self):\n        print('Type, truncation, occlusion, alpha: %s, %d, %d, %f' % \\\n              (self.type, self.truncation, self.occlusion, self.alpha))\n        print('2d bbox (x0,y0,x1,y1): %f, %f, %f, %f' % \\\n              (self.xmin, self.ymin, self.xmax, self.ymax))\n        print('3d bbox h,w,l: %f, %f, %f' % \\\n              (self.h, self.w, self.l))\n        print('3d bbox location, ry: (%f, %f, %f), %f' % \\\n              (self.t[0], self.t[1], self.t[2], self.ry))\n\n    def to_kitti_format(self):\n        kitti_str = '%s %.2f %d %.2f %.2f %.2f %.2f %.2f %.2f %.2f %.2f %.2f %.2f %.2f %.2f %.2f' \\\n                    % (self.type, self.truncation, int(self.occlusion), self.alpha, self.box2d[0], self.box2d[1],\n                       self.box2d[2], self.box2d[3], self.h, self.w, self.l, self.t[0], self.t[1], self.t[2],\n                       self.ry, self.score)\n        return kitti_str\n\n\ndef read_label(label_filename):\n    lines = [line.rstrip() for line in open(label_filename)]\n    objects = [Object3d(line) for line in lines]\n    return objects\n\n\nclass Calibration(object):\n    ''' Calibration matrices and utils\n        3d XYZ in <label>.txt are in rect camera coord.\n        2d box xy are in image2 coord\n        Points in <lidar>.bin are in Velodyne coord.\n        y_image2 = P^2_rect * x_rect\n        y_image2 = P^2_rect * R0_rect * Tr_velo_to_cam * x_velo\n        x_ref = Tr_velo_to_cam * x_velo\n        x_rect = R0_rect * x_ref\n        P^2_rect = [f^2_u,  0,      c^2_u,  -f^2_u b^2_x;\n                    0,      f^2_v,  c^2_v,  -f^2_v b^2_y;\n                    0,      0,      1,      0]\n                 = K * [1|t]\n        image2 coord:\n         ----> x-axis (u)\n        |\n        |\n        v y-axis (v)\n        velodyne coord:\n        front x, left y, up z\n        rect/ref camera coord:\n        right x, down y, front z\n        Ref (KITTI paper): http://www.cvlibs.net/publications/Geiger2013IJRR.pdf\n        TODO(rqi): do matrix multiplication only once for each projection.\n    '''\n\n    def __init__(self, calib_filepath):\n        calibs = self.read_calib_file(calib_filepath)\n        # Projection matrix from rect camera coord to image2 coord\n        self.P2 = calibs['P2']\n        self.P2 = np.reshape(self.P2, [3, 4])\n        self.P3 = calibs['P3']\n        self.P3 = np.reshape(self.P3, [3, 4])\n        # Rigid transform from Velodyne coord to reference camera coord\n        self.V2C = calibs['Tr_velo2cam']\n        self.V2C = np.reshape(self.V2C, [3, 4])\n        # Rotation from reference camera coord to rect camera coord\n        self.R0 = calibs['R_rect']\n        self.R0 = np.reshape(self.R0, [3, 3])\n\n        # Camera intrinsics and extrinsics\n        self.c_u = self.P2[0, 2]\n        self.c_v = self.P2[1, 2]\n        self.f_u = self.P2[0, 0]\n        self.f_v = self.P2[1, 1]\n        self.b_x = self.P2[0, 3] / (-self.f_u)  # relative\n        self.b_y = self.P2[1, 3] / (-self.f_v)\n\n    def read_calib_file(self, filepath):\n        with open(filepath) as f:\n            lines = f.readlines()\n\n        obj = lines[2].strip().split(' ')[1:]\n        P2 = np.array(obj, dtype=np.float32)\n        obj = lines[3].strip().split(' ')[1:]\n        P3 = np.array(obj, dtype=np.float32)\n        obj = lines[4].strip().split(' ')[1:]\n        R0 = np.array(obj, dtype=np.float32)\n        obj = lines[5].strip().split(' ')[1:]\n        Tr_velo_to_cam = np.array(obj, dtype=np.float32)\n\n        return {'P2': P2.reshape(3, 4),\n                'P3': P3.reshape(3, 4),\n                'R_rect': R0.reshape(3, 3),\n                'Tr_velo2cam': Tr_velo_to_cam.reshape(3, 4)}\n\n    def cart2hom(self, pts_3d):\n        \"\"\"\n        :param pts: (N, 3 or 2)\n        :return pts_hom: (N, 4 or 3)\n        \"\"\"\n        pts_hom = np.hstack((pts_3d, np.ones((pts_3d.shape[0], 1), dtype=np.float32)))\n        return pts_hom\n\n\ndef compute_radius(det_size, min_overlap=0.7):\n    height, width = det_size\n\n    a1 = 1\n    b1 = (height + width)\n    c1 = width * height * (1 - min_overlap) / (1 + min_overlap)\n    sq1 = np.sqrt(b1 ** 2 - 4 * a1 * c1)\n    r1 = (b1 + sq1) / 2\n\n    a2 = 4\n    b2 = 2 * (height + width)\n    c2 = (1 - min_overlap) * width * height\n    sq2 = np.sqrt(b2 ** 2 - 4 * a2 * c2)\n    r2 = (b2 + sq2) / 2\n\n    a3 = 4 * min_overlap\n    b3 = -2 * min_overlap * (height + width)\n    c3 = (min_overlap - 1) * width * height\n    sq3 = np.sqrt(b3 ** 2 - 4 * a3 * c3)\n    r3 = (b3 + sq3) / 2\n\n    return min(r1, r2, r3)\n\n\ndef gaussian2D(shape, sigma=1):\n    m, n = [(ss - 1.) / 2. for ss in shape]\n    y, x = np.ogrid[-m:m + 1, -n:n + 1]\n    h = np.exp(-(x * x + y * y) / (2 * sigma * sigma))\n    h[h < np.finfo(h.dtype).eps * h.max()] = 0\n\n    return h\n\n\ndef gen_hm_radius(heatmap, center, radius, k=1):\n    diameter = 2 * radius + 1\n    gaussian = gaussian2D((diameter, diameter), sigma=diameter / 6)\n\n    x, y = int(center[0]), int(center[1])\n\n    height, width = heatmap.shape[0:2]\n\n    left, right = min(x, radius), min(width - x, radius + 1)\n    top, bottom = min(y, radius), min(height - y, radius + 1)\n\n    masked_heatmap = heatmap[y - top:y + bottom, x - left:x + right]\n    masked_gaussian = gaussian[radius - top:radius + bottom, radius - left:radius + right]\n    if min(masked_gaussian.shape) > 0 and min(masked_heatmap.shape) > 0:  # TODO debug\n        np.maximum(masked_heatmap, masked_gaussian * k, out=masked_heatmap)\n\n    return heatmap\n\n\ndef get_filtered_lidar(lidar, boundary, labels=None):\n    minX = boundary['minX']\n    maxX = boundary['maxX']\n    minY = boundary['minY']\n    maxY = boundary['maxY']\n    minZ = boundary['minZ']\n    maxZ = boundary['maxZ']\n\n    # Remove the point out of range x,y,z\n    mask = np.where((lidar[:, 0] >= minX) & (lidar[:, 0] <= maxX) &\n                    (lidar[:, 1] >= minY) & (lidar[:, 1] <= maxY) &\n                    (lidar[:, 2] >= minZ) & (lidar[:, 2] <= maxZ))\n    lidar = lidar[mask]\n    lidar[:, 2] = lidar[:, 2] - minZ\n\n    if labels is not None:\n        label_x = (labels[:, 1] >= minX) & (labels[:, 1] < maxX)\n        label_y = (labels[:, 2] >= minY) & (labels[:, 2] < maxY)\n        label_z = (labels[:, 3] >= minZ) & (labels[:, 3] < maxZ)\n        mask_label = label_x & label_y & label_z\n        labels = labels[mask_label]\n        return lidar, labels\n    else:\n        return lidar\n\n\ndef box3d_corners_to_center(box3d_corner):\n    # (N, 8, 3) -> (N, 7)\n    assert box3d_corner.ndim == 3\n\n    xyz = np.mean(box3d_corner, axis=1)\n\n    h = abs(np.mean(box3d_corner[:, 4:, 2] - box3d_corner[:, :4, 2], axis=1, keepdims=True))\n    w = (np.sqrt(np.sum((box3d_corner[:, 0, [0, 1]] - box3d_corner[:, 1, [0, 1]]) ** 2, axis=1, keepdims=True)) +\n         np.sqrt(np.sum((box3d_corner[:, 2, [0, 1]] - box3d_corner[:, 3, [0, 1]]) ** 2, axis=1, keepdims=True)) +\n         np.sqrt(np.sum((box3d_corner[:, 4, [0, 1]] - box3d_corner[:, 5, [0, 1]]) ** 2, axis=1, keepdims=True)) +\n         np.sqrt(np.sum((box3d_corner[:, 6, [0, 1]] - box3d_corner[:, 7, [0, 1]]) ** 2, axis=1, keepdims=True))) / 4\n\n    l = (np.sqrt(np.sum((box3d_corner[:, 0, [0, 1]] - box3d_corner[:, 3, [0, 1]]) ** 2, axis=1, keepdims=True)) +\n         np.sqrt(np.sum((box3d_corner[:, 1, [0, 1]] - box3d_corner[:, 2, [0, 1]]) ** 2, axis=1, keepdims=True)) +\n         np.sqrt(np.sum((box3d_corner[:, 4, [0, 1]] - box3d_corner[:, 7, [0, 1]]) ** 2, axis=1, keepdims=True)) +\n         np.sqrt(np.sum((box3d_corner[:, 5, [0, 1]] - box3d_corner[:, 6, [0, 1]]) ** 2, axis=1, keepdims=True))) / 4\n\n    yaw = (np.arctan2(box3d_corner[:, 2, 1] - box3d_corner[:, 1, 1],\n                      box3d_corner[:, 2, 0] - box3d_corner[:, 1, 0]) +\n           np.arctan2(box3d_corner[:, 3, 1] - box3d_corner[:, 0, 1],\n                      box3d_corner[:, 3, 0] - box3d_corner[:, 0, 0]) +\n           np.arctan2(box3d_corner[:, 2, 0] - box3d_corner[:, 3, 0],\n                      box3d_corner[:, 3, 1] - box3d_corner[:, 2, 1]) +\n           np.arctan2(box3d_corner[:, 1, 0] - box3d_corner[:, 0, 0],\n                      box3d_corner[:, 0, 1] - box3d_corner[:, 1, 1]))[:, np.newaxis] / 4\n\n    return np.concatenate([h, w, l, xyz, yaw], axis=1).reshape(-1, 7)\n\n\ndef box3d_center_to_conners(box3d_center):\n    h, w, l, x, y, z, yaw = box3d_center\n    Box = np.array([[-l / 2, -l / 2, l / 2, l / 2, -l / 2, -l / 2, l / 2, l / 2],\n                    [w / 2, -w / 2, -w / 2, w / 2, w / 2, -w / 2, -w / 2, w / 2],\n                    [0, 0, 0, 0, h, h, h, h]])\n\n    rotMat = np.array([\n        [np.cos(yaw), -np.sin(yaw), 0.0],\n        [np.sin(yaw), np.cos(yaw), 0.0],\n        [0.0, 0.0, 1.0]])\n\n    velo_box = np.dot(rotMat, Box)\n    cornerPosInVelo = velo_box + np.tile(np.array([x, y, z]), (8, 1)).T\n    box3d_corner = cornerPosInVelo.transpose()\n\n    return box3d_corner.astype(np.float32)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:44:51.886643Z","iopub.execute_input":"2021-12-13T06:44:51.886897Z","iopub.status.idle":"2021-12-13T06:44:51.957034Z","shell.execute_reply.started":"2021-12-13T06:44:51.886868Z","shell.execute_reply":"2021-12-13T06:44:51.956202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Birds Eye View Utility Functions","metadata":{}},{"cell_type":"code","source":"import math\nimport os\nimport sys\n\nimport cv2\nimport numpy as np\n\n\ndef makeBEVMap(PointCloud_, boundary):\n    Height = BEV_HEIGHT + 1\n    Width = BEV_WIDTH + 1\n\n    # Discretize Feature Map\n    PointCloud = np.copy(PointCloud_)\n    PointCloud[:, 0] = np.int_(np.floor(PointCloud[:, 0] / DISCRETIZATION))\n    PointCloud[:, 1] = np.int_(np.floor(PointCloud[:, 1] / DISCRETIZATION) + Width / 2)\n\n    # sort-3times\n    sorted_indices = np.lexsort((-PointCloud[:, 2], PointCloud[:, 1], PointCloud[:, 0]))\n    PointCloud = PointCloud[sorted_indices]\n    _, unique_indices, unique_counts = np.unique(PointCloud[:, 0:2], axis=0, return_index=True, return_counts=True)\n    PointCloud_top = PointCloud[unique_indices]\n\n    # Height Map, Intensity Map & Density Map\n    heightMap = np.zeros((Height, Width))\n    intensityMap = np.zeros((Height, Width))\n    densityMap = np.zeros((Height, Width))\n\n    # some important problem is image coordinate is (y,x), not (x,y)\n    max_height = float(np.abs(boundary['maxZ'] - boundary['minZ']))\n    heightMap[np.int_(PointCloud_top[:, 0]), np.int_(PointCloud_top[:, 1])] = PointCloud_top[:, 2] / max_height\n\n    normalizedCounts = np.minimum(1.0, np.log(unique_counts + 1) / np.log(64))\n    intensityMap[np.int_(PointCloud_top[:, 0]), np.int_(PointCloud_top[:, 1])] = PointCloud_top[:, 3]\n    densityMap[np.int_(PointCloud_top[:, 0]), np.int_(PointCloud_top[:, 1])] = normalizedCounts\n\n    RGB_Map = np.zeros((3, Height - 1, Width - 1))\n    RGB_Map[2, :, :] = densityMap[:BEV_HEIGHT, :BEV_WIDTH]  # r_map\n    RGB_Map[1, :, :] = heightMap[:BEV_HEIGHT, :BEV_WIDTH]  # g_map\n    RGB_Map[0, :, :] = intensityMap[:BEV_HEIGHT, :BEV_WIDTH]  # b_map\n\n    return RGB_Map\n\n\n# bev image coordinates format\ndef get_corners(x, y, w, l, yaw):\n    bev_corners = np.zeros((4, 2), dtype=np.float32)\n    cos_yaw = np.cos(yaw)\n    sin_yaw = np.sin(yaw)\n    # front left\n    bev_corners[0, 0] = x - w / 2 * cos_yaw - l / 2 * sin_yaw\n    bev_corners[0, 1] = y - w / 2 * sin_yaw + l / 2 * cos_yaw\n\n    # rear left\n    bev_corners[1, 0] = x - w / 2 * cos_yaw + l / 2 * sin_yaw\n    bev_corners[1, 1] = y - w / 2 * sin_yaw - l / 2 * cos_yaw\n\n    # rear right\n    bev_corners[2, 0] = x + w / 2 * cos_yaw + l / 2 * sin_yaw\n    bev_corners[2, 1] = y + w / 2 * sin_yaw - l / 2 * cos_yaw\n\n    # front right\n    bev_corners[3, 0] = x + w / 2 * cos_yaw - l / 2 * sin_yaw\n    bev_corners[3, 1] = y + w / 2 * sin_yaw + l / 2 * cos_yaw\n\n    return bev_corners\n\n\ndef drawRotatedBox(img, x, y, w, l, yaw, color):\n    bev_corners = get_corners(x, y, w, l, yaw)\n    corners_int = bev_corners.reshape(-1, 1, 2).astype(int)\n    cv2.polylines(img, [corners_int], True, color, 2)\n    corners_int = bev_corners.reshape(-1, 2)\n    cv2.line(img, (int(corners_int[0, 0]), int(corners_int[0, 1])), (int(corners_int[3, 0]), int(corners_int[3, 1])), (255, 255, 0), 2)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:44:51.95859Z","iopub.execute_input":"2021-12-13T06:44:51.958918Z","iopub.status.idle":"2021-12-13T06:44:51.982073Z","shell.execute_reply.started":"2021-12-13T06:44:51.958872Z","shell.execute_reply":"2021-12-13T06:44:51.981239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Functions","metadata":{}},{"cell_type":"code","source":"import sys\nimport os\nimport math\nfrom builtins import int\n\nimport numpy as np\nfrom torch.utils.data import Dataset\nimport cv2\nimport torch\n\nclass KittiDataset(Dataset):\n    def __init__(self, configs, mode='train', lidar_aug=None, hflip_prob=None, num_samples=None):\n        self.dataset_dir = configs.dataset_dir\n        self.input_size = configs.input_size\n        self.hm_size = configs.hm_size\n\n        self.num_classes = configs.num_classes\n        self.max_objects = configs.max_objects\n\n        assert mode in ['train', 'val', 'test'], 'Invalid mode: {}'.format(mode)\n        self.mode = mode\n        self.is_test = (self.mode == 'test')\n        sub_folder = 'testing' if self.is_test else 'training'\n\n        self.lidar_aug = lidar_aug\n        self.hflip_prob = hflip_prob\n\n        self.image_dir = os.path.join(self.dataset_dir, sub_folder, \"image_2\")\n        self.lidar_dir = os.path.join(self.dataset_dir, sub_folder, \"velodyne\")\n        self.calib_dir = os.path.join(self.dataset_dir, sub_folder, \"calib\")\n        self.label_dir = os.path.join(self.dataset_dir, sub_folder, \"label_2\")\n        split_txt_path = os.path.join(configs.root_dir, 'dataset', 'kitti', 'ImageSets', '{}.txt'.format(mode))\n        self.sample_id_list = [int(x.strip()) for x in open(split_txt_path).readlines()]\n\n        if num_samples is not None:\n            self.sample_id_list = self.sample_id_list[:num_samples]\n        self.num_samples = len(self.sample_id_list)\n\n    def __len__(self):\n        return len(self.sample_id_list)\n\n    def __getitem__(self, index):\n        if self.is_test:\n            return self.load_img_only(index)\n        else:\n            return self.load_img_with_targets(index)\n\n    def load_img_only(self, index):\n        \"\"\"Load only image for the testing phase\"\"\"\n        sample_id = int(self.sample_id_list[index])\n        img_path, img_rgb = self.get_image(sample_id)\n        lidarData = self.get_lidar(sample_id)\n        lidarData = get_filtered_lidar(lidarData, boundary)\n        bev_map = makeBEVMap(lidarData, boundary)\n        bev_map = torch.from_numpy(bev_map)\n\n        metadatas = {\n            'img_path': img_path,\n        }\n\n        return metadatas, bev_map, img_rgb\n\n    def load_img_with_targets(self, index):\n        \"\"\"Load images and targets for the training and validation phase\"\"\"\n        sample_id = int(self.sample_id_list[index])\n        img_path = os.path.join(self.image_dir, '{:06d}.png'.format(sample_id))\n        lidarData = self.get_lidar(sample_id)\n        calib = self.get_calib(sample_id)\n        labels, has_labels = self.get_label(sample_id)\n        if has_labels:\n            labels[:, 1:] = camera_to_lidar_box(labels[:, 1:], calib.V2C, calib.R0, calib.P2)\n\n        if self.lidar_aug:\n            lidarData, labels[:, 1:] = self.lidar_aug(lidarData, labels[:, 1:])\n\n        lidarData, labels = get_filtered_lidar(lidarData, boundary, labels)\n\n        bev_map = makeBEVMap(lidarData, boundary)\n        bev_map = torch.from_numpy(bev_map)\n\n        hflipped = False\n        if np.random.random() < self.hflip_prob:\n            hflipped = True\n            # C, H, W\n            bev_map = torch.flip(bev_map, [-1])\n\n        targets = self.build_targets(labels, hflipped)\n\n        metadatas = {\n            'img_path': img_path,\n            'hflipped': hflipped\n        }\n\n        return metadatas, bev_map, targets\n\n    def get_image(self, idx):\n        img_path = os.path.join(self.image_dir, '{:06d}.png'.format(idx))\n        img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n\n        return img_path, img\n\n    def get_calib(self, idx):\n        calib_file = os.path.join(self.calib_dir, '{:06d}.txt'.format(idx))\n        # assert os.path.isfile(calib_file)\n        return Calibration(calib_file)\n\n    def get_lidar(self, idx):\n        lidar_file = os.path.join(self.lidar_dir, '{:06d}.bin'.format(idx))\n        # assert os.path.isfile(lidar_file)\n        return np.fromfile(lidar_file, dtype=np.float32).reshape(-1, 4)\n\n    def get_label(self, idx):\n        labels = []\n        label_path = os.path.join(self.label_dir, '{:06d}.txt'.format(idx))\n        for line in open(label_path, 'r'):\n            line = line.rstrip()\n            line_parts = line.split(' ')\n            obj_name = line_parts[0]  # 'Car', 'Pedestrian', ...\n            cat_id = int(CLASS_NAME_TO_ID[obj_name])\n            if cat_id <= -99:  # ignore Tram and Misc\n                continue\n            truncated = int(float(line_parts[1]))  # truncated pixel ratio [0..1]\n            occluded = int(line_parts[2])  # 0=visible, 1=partly occluded, 2=fully occluded, 3=unknown\n            alpha = float(line_parts[3])  # object observation angle [-pi..pi]\n            # xmin, ymin, xmax, ymax\n            bbox = np.array([float(line_parts[4]), float(line_parts[5]), float(line_parts[6]), float(line_parts[7])])\n            # height, width, length (h, w, l)\n            h, w, l = float(line_parts[8]), float(line_parts[9]), float(line_parts[10])\n            # location (x,y,z) in camera coord.\n            x, y, z = float(line_parts[11]), float(line_parts[12]), float(line_parts[13])\n            ry = float(line_parts[14])  # yaw angle (around Y-axis in camera coordinates) [-pi..pi]\n\n            object_label = [cat_id, x, y, z, h, w, l, ry]\n            labels.append(object_label)\n\n        if len(labels) == 0:\n            labels = np.zeros((1, 8), dtype=np.float32)\n            has_labels = False\n        else:\n            labels = np.array(labels, dtype=np.float32)\n            has_labels = True\n\n        return labels, has_labels\n\n    def build_targets(self, labels, hflipped):\n        minX = boundary['minX']\n        maxX = boundary['maxX']\n        minY = boundary['minY']\n        maxY = boundary['maxY']\n        minZ = boundary['minZ']\n        maxZ = boundary['maxZ']\n\n        num_objects = min(len(labels), self.max_objects)\n        hm_l, hm_w = self.hm_size\n\n        hm_main_center = np.zeros((self.num_classes, hm_l, hm_w), dtype=np.float32)\n        cen_offset = np.zeros((self.max_objects, 2), dtype=np.float32)\n        direction = np.zeros((self.max_objects, 2), dtype=np.float32)\n        z_coor = np.zeros((self.max_objects, 1), dtype=np.float32)\n        dimension = np.zeros((self.max_objects, 3), dtype=np.float32)\n\n        indices_center = np.zeros((self.max_objects), dtype=np.int64)\n        obj_mask = np.zeros((self.max_objects), dtype=np.uint8)\n\n        for k in range(num_objects):\n            cls_id, x, y, z, h, w, l, yaw = labels[k]\n            cls_id = int(cls_id)\n            # Invert yaw angle\n            yaw = -yaw\n            if not ((minX <= x <= maxX) and (minY <= y <= maxY) and (minZ <= z <= maxZ)):\n                continue\n            if (h <= 0) or (w <= 0) or (l <= 0):\n                continue\n\n            bbox_l = l / bound_size_x * hm_l\n            bbox_w = w / bound_size_y * hm_w\n            radius = compute_radius((math.ceil(bbox_l), math.ceil(bbox_w)))\n            radius = max(0, int(radius))\n\n            center_y = (x - minX) / bound_size_x * hm_l  # x --> y (invert to 2D image space)\n            center_x = (y - minY) / bound_size_y * hm_w  # y --> x\n            center = np.array([center_x, center_y], dtype=np.float32)\n\n            if hflipped:\n                center[0] = hm_w - center[0] - 1\n\n            center_int = center.astype(np.int32)\n            if cls_id < 0:\n                ignore_ids = [_ for _ in range(self.num_classes)] if cls_id == - 1 else [- cls_id - 2]\n                # Consider to make mask ignore\n                for cls_ig in ignore_ids:\n                    gen_hm_radius(hm_main_center[cls_ig], center_int, radius)\n                hm_main_center[ignore_ids, center_int[1], center_int[0]] = 0.9999\n                continue\n\n            # Generate heatmaps for main center\n            gen_hm_radius(hm_main_center[cls_id], center, radius)\n            # Index of the center\n            indices_center[k] = center_int[1] * hm_w + center_int[0]\n\n            # targets for center offset\n            cen_offset[k] = center - center_int\n\n            # targets for dimension\n            dimension[k, 0] = h\n            dimension[k, 1] = w\n            dimension[k, 2] = l\n\n            # targets for direction\n            direction[k, 0] = math.sin(float(yaw))  # im\n            direction[k, 1] = math.cos(float(yaw))  # re\n            # im -->> -im\n            if hflipped:\n                direction[k, 0] = - direction[k, 0]\n\n            # targets for depth\n            z_coor[k] = z - minZ\n\n            # Generate object masks\n            obj_mask[k] = 1\n\n        targets = {\n            'hm_cen': hm_main_center,\n            'cen_offset': cen_offset,\n            'direction': direction,\n            'z_coor': z_coor,\n            'dim': dimension,\n            'indices_center': indices_center,\n            'obj_mask': obj_mask,\n        }\n\n        return targets\n\n    def draw_img_with_label(self, index):\n        sample_id = int(self.sample_id_list[index])\n        img_path, img_rgb = self.get_image(sample_id)\n        lidarData = self.get_lidar(sample_id)\n        calib = self.get_calib(sample_id)\n        labels, has_labels = self.get_label(sample_id)\n        if has_labels:\n            labels[:, 1:] = camera_to_lidar_box(labels[:, 1:], calib.V2C, calib.R0, calib.P2)\n\n        if self.lidar_aug:\n            lidarData, labels[:, 1:] = self.lidar_aug(lidarData, labels[:, 1:])\n\n        lidarData, labels = get_filtered_lidar(lidarData, boundary, labels)\n        bev_map = makeBEVMap(lidarData, boundary)\n\n        return bev_map, labels, img_rgb, img_path","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:44:51.984575Z","iopub.execute_input":"2021-12-13T06:44:51.985144Z","iopub.status.idle":"2021-12-13T06:44:52.037895Z","shell.execute_reply.started":"2021-12-13T06:44:51.985096Z","shell.execute_reply":"2021-12-13T06:44:52.037046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Loader","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\n\nimport torch\nfrom torch.utils.data import DataLoader\nimport numpy as np\n\n\ndef create_train_dataloader(configs):\n    \"\"\"Create dataloader for training\"\"\"\n    train_lidar_aug = OneOf([\n        Random_Rotation(limit_angle=np.pi / 4, p=1.0),\n        Random_Scaling(scaling_range=(0.95, 1.05), p=1.0),\n    ], p=0.66)\n    train_dataset = KittiDataset(configs, mode='train', lidar_aug=train_lidar_aug, hflip_prob=configs.hflip_prob,\n                                 num_samples=configs.num_samples)\n    train_sampler = None\n    if configs.distributed:\n        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, batch_size=configs.batch_size, shuffle=(train_sampler is None),\n                                  pin_memory=configs.pin_memory, num_workers=configs.num_workers, sampler=train_sampler)\n\n    return train_dataloader, train_sampler\n\n\ndef create_val_dataloader(configs):\n    \"\"\"Create dataloader for validation\"\"\"\n    val_sampler = None\n    val_dataset = KittiDataset(configs, mode='val', lidar_aug=None, hflip_prob=0., num_samples=configs.num_samples)\n    if configs.distributed:\n        val_sampler = torch.utils.data.distributed.DistributedSampler(val_dataset, shuffle=False)\n    val_dataloader = DataLoader(val_dataset, batch_size=configs.batch_size, shuffle=False,\n                                pin_memory=configs.pin_memory, num_workers=configs.num_workers, sampler=val_sampler)\n\n    return val_dataloader\n\n\ndef create_test_dataloader(configs):\n    \"\"\"Create dataloader for testing phase\"\"\"\n\n    test_dataset = KittiDataset(configs, mode='test', lidar_aug=None, hflip_prob=0., num_samples=configs.num_samples)\n    test_sampler = None\n    if configs.distributed:\n        test_sampler = torch.utils.data.distributed.DistributedSampler(test_dataset)\n    test_dataloader = DataLoader(test_dataset, batch_size=configs.batch_size, shuffle=False,\n                                 pin_memory=configs.pin_memory, num_workers=configs.num_workers, sampler=test_sampler)\n\n    return test_dataloader","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:44:52.039397Z","iopub.execute_input":"2021-12-13T06:44:52.039682Z","iopub.status.idle":"2021-12-13T06:44:52.055729Z","shell.execute_reply.started":"2021-12-13T06:44:52.039643Z","shell.execute_reply":"2021-12-13T06:44:52.054831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utility Functions","metadata":{}},{"cell_type":"markdown","source":"## Logger","metadata":{}},{"cell_type":"code","source":"import os\nimport logging\n\n\nclass Logger():\n    \"\"\"\n        Create logger to save logs during training\n        Args:\n            logs_dir:\n            saved_fn:\n        Returns:\n        \"\"\"\n\n    def __init__(self, logs_dir, saved_fn):\n        logger_fn = 'logger_{}.txt'.format(saved_fn)\n        logger_path = os.path.join(logs_dir, logger_fn)\n\n        self.logger = logging.getLogger(__name__)\n        self.logger.setLevel(logging.INFO)\n\n        # formatter = logging.Formatter('%(asctime)s:File %(module)s.py:Func %(funcName)s:Line %(lineno)d:%(levelname)s: %(message)s')\n        formatter = logging.Formatter(\n            '%(asctime)s: %(module)s.py - %(funcName)s(), at Line %(lineno)d:%(levelname)s:\\n%(message)s')\n\n        file_handler = logging.FileHandler(logger_path)\n        file_handler.setLevel(logging.INFO)\n        file_handler.setFormatter(formatter)\n\n        stream_handler = logging.StreamHandler()\n        stream_handler.setFormatter(formatter)\n\n        self.logger.addHandler(file_handler)\n        self.logger.addHandler(stream_handler)\n\n    def info(self, message):\n        self.logger.info(message)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:44:52.057614Z","iopub.execute_input":"2021-12-13T06:44:52.058016Z","iopub.status.idle":"2021-12-13T06:44:52.069234Z","shell.execute_reply.started":"2021-12-13T06:44:52.057978Z","shell.execute_reply":"2021-12-13T06:44:52.068478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pytorch Utilities","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.distributed as dist\n\n__all__ = ['convert2cpu', 'convert2cpu_long', 'to_cpu', 'reduce_tensor', 'to_python_float', '_sigmoid']\n\n\ndef convert2cpu(gpu_matrix):\n    return torch.FloatTensor(gpu_matrix.size()).copy_(gpu_matrix)\n\n\ndef convert2cpu_long(gpu_matrix):\n    return torch.LongTensor(gpu_matrix.size()).copy_(gpu_matrix)\n\n\ndef to_cpu(tensor):\n    return tensor.detach().cpu()\n\n\ndef reduce_tensor(tensor, world_size):\n    rt = tensor.clone()\n    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n    rt /= world_size\n    return rt\n\n\ndef to_python_float(t):\n    if hasattr(t, 'item'):\n        return t.item()\n    else:\n        return t[0]\n\n\ndef _sigmoid(x):\n    return torch.clamp(x.sigmoid_(), min=1e-4, max=1 - 1e-4)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:44:52.070961Z","iopub.execute_input":"2021-12-13T06:44:52.071351Z","iopub.status.idle":"2021-12-13T06:44:52.083055Z","shell.execute_reply.started":"2021-12-13T06:44:52.07131Z","shell.execute_reply":"2021-12-13T06:44:52.08203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Miscellaneous","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport time\n\ndef make_folder(folder_name):\n    if not os.path.exists(folder_name):\n        os.makedirs(folder_name)\n    # or os.makedirs(folder_name, exist_ok=True)\n\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self, name, fmt=':f'):\n        self.name = name\n        self.fmt = fmt\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)\n\n\nclass ProgressMeter(object):\n    def __init__(self, num_batches, meters, prefix=\"\"):\n        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n        self.meters = meters\n        self.prefix = prefix\n\n    def display(self, batch):\n        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n        entries += [str(meter) for meter in self.meters]\n        print('\\t'.join(entries))\n\n    def get_message(self, batch):\n        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n        entries += [str(meter) for meter in self.meters]\n        return '\\t'.join(entries)\n\n    def _get_batch_fmtstr(self, num_batches):\n        num_digits = len(str(num_batches // 1))\n        fmt = '{:' + str(num_digits) + 'd}'\n        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n\n\ndef time_synchronized():\n    torch.cuda.synchronize() if torch.cuda.is_available() else None\n    return time.time()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:44:52.0847Z","iopub.execute_input":"2021-12-13T06:44:52.085066Z","iopub.status.idle":"2021-12-13T06:44:52.101126Z","shell.execute_reply.started":"2021-12-13T06:44:52.08503Z","shell.execute_reply":"2021-12-13T06:44:52.100229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Learning Rate Scheduler","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.optim import SGD, lr_scheduler\nimport numpy as np\n\n\nclass _LRMomentumScheduler(lr_scheduler._LRScheduler):\n    def __init__(self, optimizer, last_epoch=-1):\n        if last_epoch == -1:\n            for group in optimizer.param_groups:\n                group.setdefault('initial_momentum', group['momentum'])\n        else:\n            for i, group in enumerate(optimizer.param_groups):\n                if 'initial_momentum' not in group:\n                    raise KeyError(\"param 'initial_momentum' is not specified \"\n                                   \"in param_groups[{}] when resuming an optimizer\".format(i))\n        self.base_momentums = list(map(lambda group: group['initial_momentum'], optimizer.param_groups))\n        super().__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        raise NotImplementedError\n\n    def get_momentum(self):\n        raise NotImplementedError\n\n    def step(self, epoch=None):\n        if epoch is None:\n            epoch = self.last_epoch + 1\n        self.last_epoch = epoch\n        for param_group, lr, momentum in zip(self.optimizer.param_groups, self.get_lr(), self.get_momentum()):\n            param_group['lr'] = lr\n            param_group['momentum'] = momentum\n\n\nclass ParameterUpdate(object):\n    \"\"\"A callable class used to define an arbitrary schedule defined by a list.\n    This object is designed to be passed to the LambdaLR or LambdaScheduler scheduler to apply\n    the given schedule.\n    Arguments:\n        params {list or numpy.array} -- List or numpy array defining parameter schedule.\n        base_param {float} -- Parameter value used to initialize the optimizer.\n    \"\"\"\n\n    def __init__(self, params, base_param):\n        self.params = np.hstack([params, 0])\n        self.base_param = base_param\n\n    def __call__(self, epoch):\n        return self.params[epoch] / self.base_param\n\n\ndef apply_lambda(last_epoch, bases, lambdas):\n    return [base * lmbda(last_epoch) for lmbda, base in zip(lambdas, bases)]\n\n\nclass LambdaScheduler(_LRMomentumScheduler):\n    \"\"\"Sets the learning rate and momentum of each parameter group to the initial lr and momentum\n    times a given function. When last_epoch=-1, sets initial lr and momentum to the optimizer\n    values.\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        lr_lambda (function or list): A function which computes a multiplicative\n            factor given an integer parameter epoch, or a list of such\n            functions, one for each group in optimizer.param_groups.\n            Default: lambda x:x.\n        momentum_lambda (function or list): As for lr_lambda but applied to momentum.\n            Default: lambda x:x.\n        last_epoch (int): The index of last epoch. Default: -1.\n    Example:\n        >>> # Assuming optimizer has two groups.\n        >>> lr_lambda = [\n        ...     lambda epoch: epoch // 30,\n        ...     lambda epoch: 0.95 ** epoch\n        ... ]\n        >>> mom_lambda = [\n        ...     lambda epoch: max(0, (50 - epoch) // 50),\n        ...     lambda epoch: 0.99 ** epoch\n        ... ]\n        >>> scheduler = LambdaScheduler(optimizer, lr_lambda, mom_lambda)\n        >>> for epoch in range(100):\n        >>>     train(...)\n        >>>     validate(...)\n        >>>     scheduler.step()\n    \"\"\"\n\n    def __init__(self, optimizer, lr_lambda=lambda x: x, momentum_lambda=lambda x: x, last_epoch=-1):\n        self.optimizer = optimizer\n\n        if not isinstance(lr_lambda, (list, tuple)):\n            self.lr_lambdas = [lr_lambda] * len(optimizer.param_groups)\n        else:\n            if len(lr_lambda) != len(optimizer.param_groups):\n                raise ValueError(\"Expected {} lr_lambdas, but got {}\".format(\n                    len(optimizer.param_groups), len(lr_lambda)))\n            self.lr_lambdas = list(lr_lambda)\n\n        if not isinstance(momentum_lambda, (list, tuple)):\n            self.momentum_lambdas = [momentum_lambda] * len(optimizer.param_groups)\n        else:\n            if len(momentum_lambda) != len(optimizer.param_groups):\n                raise ValueError(\"Expected {} momentum_lambdas, but got {}\".format(\n                    len(optimizer.param_groups), len(momentum_lambda)))\n            self.momentum_lambdas = list(momentum_lambda)\n\n        self.last_epoch = last_epoch\n        super().__init__(optimizer, last_epoch)\n\n    def state_dict(self):\n        \"\"\"Returns the state of the scheduler as a :class:`dict`.\n        It contains an entry for every variable in self.__dict__ which\n        is not the optimizer.\n        The learning rate and momentum lambda functions will only be saved if they are\n        callable objects and not if they are functions or lambdas.\n        \"\"\"\n        state_dict = {key: value for key, value in self.__dict__.items()\n                      if key not in ('optimizer', 'lr_lambdas', 'momentum_lambdas')}\n        state_dict['lr_lambdas'] = [None] * len(self.lr_lambdas)\n        state_dict['momentum_lambdas'] = [None] * len(self.momentum_lambdas)\n\n        for idx, (lr_fn, mom_fn) in enumerate(zip(self.lr_lambdas, self.momentum_lambdas)):\n            if not isinstance(lr_fn, types.FunctionType):\n                state_dict['lr_lambdas'][idx] = lr_fn.__dict__.copy()\n            if not isinstance(mom_fn, types.FunctionType):\n                state_dict['momentum_lambdas'][idx] = mom_fn.__dict__.copy()\n\n        return state_dict\n\n    def load_state_dict(self, state_dict):\n        \"\"\"Loads the schedulers state.\n        Arguments:\n            state_dict (dict): scheduler state. Should be an object returned\n                from a call to :meth:`state_dict`.\n        \"\"\"\n        lr_lambdas = state_dict.pop('lr_lambdas')\n        momentum_lambdas = state_dict.pop('momentum_lambdas')\n        self.__dict__.update(state_dict)\n\n        for idx, fn in enumerate(lr_lambdas):\n            if fn is not None:\n                self.lr_lambdas[idx].__dict__.update(fn)\n\n        for idx, fn in enumerate(momentum_lambdas):\n            if fn is not None:\n                self.momentum_lambdas[idx].__dict__.update(fn)\n\n    def get_lr(self):\n        return apply_lambda(self.last_epoch, self.base_lrs, self.lr_lambdas)\n\n    def get_momentum(self):\n        return apply_lambda(self.last_epoch, self.base_momentums, self.momentum_lambdas)\n\n\nclass ParameterUpdate(object):\n    \"\"\"A callable class used to define an arbitrary schedule defined by a list.\n    This object is designed to be passed to the LambdaLR or LambdaScheduler scheduler to apply\n    the given schedule. If a base_param is zero, no updates are applied.\n    Arguments:\n        params {list or numpy.array} -- List or numpy array defining parameter schedule.\n        base_param {float} -- Parameter value used to initialize the optimizer.\n    \"\"\"\n\n    def __init__(self, params, base_param):\n        self.params = np.hstack([params, 0])\n        self.base_param = base_param\n\n        if base_param < 1e-12:\n            self.base_param = 1\n            self.params = self.params * 0.0 + 1.0\n\n    def __call__(self, epoch):\n        return self.params[epoch] / self.base_param\n\n\nclass ListScheduler(LambdaScheduler):\n    \"\"\"Sets the learning rate and momentum of each parameter group to values defined by lists.\n    When last_epoch=-1, sets initial lr and momentum to the optimizer values. One of both of lr\n    and momentum schedules may be specified.\n    Note that the parameters used to initialize the optimizer are overriden by those defined by\n    this scheduler.\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        lrs (list or numpy.ndarray): A list of learning rates, or a list of lists, one for each\n            parameter group. One- or two-dimensional numpy arrays may also be passed.\n        momentum (list or numpy.ndarray): A list of momentums, or a list of lists, one for each\n            parameter group. One- or two-dimensional numpy arrays may also be passed.\n        last_epoch (int): The index of last epoch. Default: -1.\n    Example:\n        >>> # Assuming optimizer has two groups.\n        >>> lrs = [\n        ...     np.linspace(0.01, 0.1, 100),\n        ...     np.logspace(-2, 0, 100)\n        ... ]\n        >>> momentums = [\n        ...     np.linspace(0.85, 0.95, 100),\n        ...     np.linspace(0.8, 0.99, 100)\n        ... ]\n        >>> scheduler = ListScheduler(optimizer, lrs, momentums)\n        >>> for epoch in range(100):\n        >>>     train(...)\n        >>>     validate(...)\n        >>>     scheduler.step()\n    \"\"\"\n\n    def __init__(self, optimizer, lrs=None, momentums=None, last_epoch=-1):\n        groups = optimizer.param_groups\n        if lrs is None:\n            lr_lambda = lambda x: x\n        else:\n            lrs = np.array(lrs) if isinstance(lrs, (list, tuple)) else lrs\n            if len(lrs.shape) == 1:\n                lr_lambda = [ParameterUpdate(lrs, g['lr']) for g in groups]\n            else:\n                lr_lambda = [ParameterUpdate(l, g['lr']) for l, g in zip(lrs, groups)]\n\n        if momentums is None:\n            momentum_lambda = lambda x: x\n        else:\n            momentums = np.array(momentums) if isinstance(momentums, (list, tuple)) else momentums\n            if len(momentums.shape) == 1:\n                momentum_lambda = [ParameterUpdate(momentums, g['momentum']) for g in groups]\n            else:\n                momentum_lambda = [ParameterUpdate(l, g['momentum']) for l, g in zip(momentums, groups)]\n        super().__init__(optimizer, lr_lambda, momentum_lambda)\n\n\nclass RangeFinder(ListScheduler):\n    \"\"\"Scheduler class that implements the LR range search specified in:\n        A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch\n        size, momentum, and weight decay. Leslie N. Smith, 2018, arXiv:1803.09820.\n    Logarithmically spaced learning rates from 1e-7 to 1 are searched. The number of increments in\n    that range is determined by 'epochs'.\n    Note that the parameters used to initialize the optimizer are overriden by those defined by\n    this scheduler.\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        epochs (int): Number of epochs over which to run test.\n    Example:\n        >>> scheduler = RangeFinder(optimizer, 100)\n        >>> for epoch in range(100):\n        >>>     train(...)\n        >>>     validate(...)\n        >>>     scheduler.step()\n    \"\"\"\n\n    def __init__(self, optimizer, epochs):\n        lrs = np.logspace(-7, 0, epochs)\n        super().__init__(optimizer, lrs)\n\n\nclass OneCyclePolicy(ListScheduler):\n    \"\"\"Scheduler class that implements the 1cycle policy search specified in:\n        A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch\n        size, momentum, and weight decay. Leslie N. Smith, 2018, arXiv:1803.09820.\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        lr (float or list). Maximum learning rate in range. If a list of values is passed, they\n            should correspond to parameter groups.\n        epochs (int): The number of epochs to use during search.\n        momentum_rng (list). Optional upper and lower momentum values (may be both equal). Set to\n            None to run without momentum. Default: [0.85, 0.95]. If a list of lists is passed, they\n            should correspond to parameter groups.\n        phase_ratio (float): Fraction of epochs used for the increasing and decreasing phase of\n            the schedule. For example, if phase_ratio=0.45 and epochs=100, the learning rate will\n            increase from lr/10 to lr over 45 epochs, then decrease back to lr/10 over 45 epochs,\n            then decrease to lr/100 over the remaining 10 epochs. Default: 0.45.\n    \"\"\"\n\n    def __init__(self, optimizer, lr, epochs, momentum_rng=[0.85, 0.95], phase_ratio=0.45):\n        phase_epochs = int(phase_ratio * epochs)\n        if isinstance(lr, (list, tuple)):\n            lrs = [\n                np.hstack([\n                    np.linspace(l * 1e-1, l, phase_epochs),\n                    np.linspace(l, l * 1e-1, phase_epochs),\n                    np.linspace(l * 1e-1, l * 1e-2, epochs - 2 * phase_epochs),\n                ]) for l in lr\n            ]\n        else:\n            lrs = np.hstack([\n                np.linspace(lr * 1e-1, lr, phase_epochs),\n                np.linspace(lr, lr * 1e-1, phase_epochs),\n                np.linspace(lr * 1e-1, lr * 1e-2, epochs - 2 * phase_epochs),\n            ])\n\n        if momentum_rng is not None:\n            momentum_rng = np.array(momentum_rng)\n            if len(momentum_rng.shape) == 2:\n                for i, g in enumerate(optimizer.param_groups):\n                    g['momentum'] = momentum_rng[i][1]\n                momentums = [\n                    np.hstack([\n                        np.linspace(m[1], m[0], phase_epochs),\n                        np.linspace(m[0], m[1], phase_epochs),\n                        np.linspace(m[1], m[1], epochs - 2 * phase_epochs),\n                    ]) for m in momentum_rng\n                ]\n            else:\n                for i, g in enumerate(optimizer.param_groups):\n                    g['momentum'] = momentum_rng[1]\n                momentums = np.hstack([\n                    np.linspace(momentum_rng[1], momentum_rng[0], phase_epochs),\n                    np.linspace(momentum_rng[0], momentum_rng[1], phase_epochs),\n                    np.linspace(momentum_rng[1], momentum_rng[1], epochs - 2 * phase_epochs),\n                ])\n        else:\n            momentums = None\n\n        super().__init__(optimizer, lrs, momentums)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:44:52.106369Z","iopub.execute_input":"2021-12-13T06:44:52.106619Z","iopub.status.idle":"2021-12-13T06:44:52.154822Z","shell.execute_reply.started":"2021-12-13T06:44:52.106593Z","shell.execute_reply":"2021-12-13T06:44:52.154014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Utility Functions","metadata":{}},{"cell_type":"code","source":"import copy\nimport os\nimport math\nimport sys\n\nimport torch\nfrom torch.optim.lr_scheduler import LambdaLR\nimport matplotlib.pyplot as plt\n\n\ndef create_optimizer(configs, model):\n    \"\"\"Create optimizer for training process\n    \"\"\"\n    if hasattr(model, 'module'):\n        train_params = [param for param in model.module.parameters() if param.requires_grad]\n    else:\n        train_params = [param for param in model.parameters() if param.requires_grad]\n\n    if configs.optimizer_type == 'sgd':\n        optimizer = torch.optim.SGD(train_params, lr=configs.lr, momentum=configs.momentum, nesterov=True)\n    elif configs.optimizer_type == 'adam':\n        optimizer = torch.optim.Adam(train_params, lr=configs.lr, weight_decay=configs.weight_decay)\n    else:\n        assert False, \"Unknown optimizer type\"\n\n    return optimizer\n\n\ndef create_lr_scheduler(optimizer, configs):\n    \"\"\"Create learning rate scheduler for training process\"\"\"\n\n    if configs.lr_type == 'multi_step':\n        def multi_step_scheduler(i):\n            if i < configs.steps[0]:\n                factor = 1.\n            elif i < configs.steps[1]:\n                factor = 0.1\n            else:\n                factor = 0.01\n\n            return factor\n\n        lr_scheduler = LambdaLR(optimizer, multi_step_scheduler)\n\n    elif configs.lr_type == 'cosin':\n        # Scheduler https://arxiv.org/pdf/1812.01187.pdf\n        lf = lambda x: (((1 + math.cos(x * math.pi / configs.num_epochs)) / 2) ** 1.0) * 0.9 + 0.1  # cosine\n        lr_scheduler = LambdaLR(optimizer, lr_lambda=lf)\n    elif configs.lr_type == 'one_cycle':\n        lr_scheduler = OneCyclePolicy(optimizer, configs.lr, configs.num_epochs, momentum_rng=[0.85, 0.95],\n                                      phase_ratio=0.45)\n    else:\n        raise ValueError\n\n    plot_lr_scheduler(optimizer, lr_scheduler, configs.num_epochs, save_dir=configs.logs_dir, lr_type=configs.lr_type)\n\n    return lr_scheduler\n\n\ndef get_saved_state(model, optimizer, lr_scheduler, epoch, configs):\n    \"\"\"Get the information to save with checkpoints\"\"\"\n    if hasattr(model, 'module'):\n        model_state_dict = model.module.state_dict()\n    else:\n        model_state_dict = model.state_dict()\n    utils_state_dict = {\n        'epoch': epoch,\n        'configs': configs,\n        'optimizer': copy.deepcopy(optimizer.state_dict()),\n        'lr_scheduler': copy.deepcopy(lr_scheduler.state_dict())\n    }\n\n    return model_state_dict, utils_state_dict\n\n\ndef save_checkpoint(checkpoints_dir, saved_fn, model_state_dict, utils_state_dict, epoch):\n    \"\"\"Save checkpoint every epoch only is best model or after every checkpoint_freq epoch\"\"\"\n    model_save_path = os.path.join(checkpoints_dir, 'Model_{}_epoch_{}.pth'.format(saved_fn, epoch))\n    utils_save_path = os.path.join(checkpoints_dir, 'Utils_{}_epoch_{}.pth'.format(saved_fn, epoch))\n\n    torch.save(model_state_dict, model_save_path)\n    torch.save(utils_state_dict, utils_save_path)\n\n    print('save a checkpoint at {}'.format(model_save_path))\n\n\ndef plot_lr_scheduler(optimizer, scheduler, num_epochs=300, save_dir='', lr_type=''):\n    # Plot LR simulating training for full num_epochs\n    optimizer, scheduler = copy.copy(optimizer), copy.copy(scheduler)  # do not modify originals\n    y = []\n    for _ in range(num_epochs):\n        scheduler.step()\n        y.append(optimizer.param_groups[0]['lr'])\n    plt.plot(y, '.-', label='LR')\n    plt.xlabel('epoch')\n    plt.ylabel('LR')\n    plt.grid()\n    plt.xlim(0, num_epochs)\n    plt.ylim(0)\n    plt.tight_layout()\n    plt.savefig(os.path.join(save_dir, 'LR_{}.png'.format(lr_type)), dpi=200)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:44:52.157247Z","iopub.execute_input":"2021-12-13T06:44:52.157918Z","iopub.status.idle":"2021-12-13T06:44:52.178004Z","shell.execute_reply.started":"2021-12-13T06:44:52.157877Z","shell.execute_reply":"2021-12-13T06:44:52.1772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation Utility Functions","metadata":{}},{"cell_type":"code","source":"from __future__ import division\nimport os\nimport sys\n\nimport torch\nimport numpy as np\nimport torch.nn.functional as F\nimport cv2\n\ndef _nms(heat, kernel=3):\n    pad = (kernel - 1) // 2\n    hmax = F.max_pool2d(heat, (kernel, kernel), stride=1, padding=pad)\n    keep = (hmax == heat).float()\n\n    return heat * keep\n\n\ndef _gather_feat(feat, ind, mask=None):\n    dim = feat.size(2)\n    ind = ind.unsqueeze(2).expand(ind.size(0), ind.size(1), dim)\n    feat = feat.gather(1, ind)\n    if mask is not None:\n        mask = mask.unsqueeze(2).expand_as(feat)\n        feat = feat[mask]\n        feat = feat.view(-1, dim)\n    return feat\n\n\ndef _transpose_and_gather_feat(feat, ind):\n    feat = feat.permute(0, 2, 3, 1).contiguous()\n    feat = feat.view(feat.size(0), -1, feat.size(3))\n    feat = _gather_feat(feat, ind)\n    return feat\n\n\ndef _topk(scores, K=40):\n    batch, cat, height, width = scores.size()\n\n    topk_scores, topk_inds = torch.topk(scores.view(batch, cat, -1), K)\n\n    topk_inds = topk_inds % (height * width)\n    topk_ys = (torch.floor_divide(topk_inds, width)).float()\n    topk_xs = (topk_inds % width).int().float()\n\n    topk_score, topk_ind = torch.topk(topk_scores.view(batch, -1), K)\n    topk_clses = (torch.floor_divide(topk_ind, K)).int()\n    topk_inds = _gather_feat(topk_inds.view(batch, -1, 1), topk_ind).view(batch, K)\n    topk_ys = _gather_feat(topk_ys.view(batch, -1, 1), topk_ind).view(batch, K)\n    topk_xs = _gather_feat(topk_xs.view(batch, -1, 1), topk_ind).view(batch, K)\n\n    return topk_score, topk_inds, topk_clses, topk_ys, topk_xs\n\n\ndef _topk_channel(scores, K=40):\n    batch, cat, height, width = scores.size()\n\n    topk_scores, topk_inds = torch.topk(scores.view(batch, cat, -1), K)\n\n    topk_inds = topk_inds % (height * width)\n    topk_ys = (topk_inds / width).int().float()\n    topk_xs = (topk_inds % width).int().float()\n\n    return topk_scores, topk_inds, topk_ys, topk_xs\n\n\ndef decode(hm_cen, cen_offset, direction, z_coor, dim, K=40):\n    batch_size, num_classes, height, width = hm_cen.size()\n\n    hm_cen = _nms(hm_cen)\n    scores, inds, clses, ys, xs = _topk(hm_cen, K=K)\n    if cen_offset is not None:\n        cen_offset = _transpose_and_gather_feat(cen_offset, inds)\n        cen_offset = cen_offset.view(batch_size, K, 2)\n        xs = xs.view(batch_size, K, 1) + cen_offset[:, :, 0:1]\n        ys = ys.view(batch_size, K, 1) + cen_offset[:, :, 1:2]\n    else:\n        xs = xs.view(batch_size, K, 1) + 0.5\n        ys = ys.view(batch_size, K, 1) + 0.5\n\n    direction = _transpose_and_gather_feat(direction, inds)\n    direction = direction.view(batch_size, K, 2)\n    z_coor = _transpose_and_gather_feat(z_coor, inds)\n    z_coor = z_coor.view(batch_size, K, 1)\n    dim = _transpose_and_gather_feat(dim, inds)\n    dim = dim.view(batch_size, K, 3)\n    clses = clses.view(batch_size, K, 1).float()\n    scores = scores.view(batch_size, K, 1)\n\n    # (scores x 1, ys x 1, xs x 1, z_coor x 1, dim x 3, direction x 2, clses x 1)\n    # (scores-0:1, ys-1:2, xs-2:3, z_coor-3:4, dim-4:7, direction-7:9, clses-9:10)\n    # detections: [batch_size, K, 10]\n    detections = torch.cat([scores, xs, ys, z_coor, dim, direction, clses], dim=2)\n\n    return detections\n\n\ndef get_yaw(direction):\n    return np.arctan2(direction[:, 0:1], direction[:, 1:2])\n\n\ndef post_processing(detections, num_classes=3, down_ratio=4, peak_thresh=0.2):\n    \"\"\"\n    :param detections: [batch_size, K, 10]\n    # (scores x 1, xs x 1, ys x 1, z_coor x 1, dim x 3, direction x 2, clses x 1)\n    # (scores-0:1, xs-1:2, ys-2:3, z_coor-3:4, dim-4:7, direction-7:9, clses-9:10)\n    :return:\n    \"\"\"\n    # TODO: Need to consider rescale to the original scale: x, y\n\n    ret = []\n    for i in range(detections.shape[0]):\n        top_preds = {}\n        classes = detections[i, :, -1]\n        for j in range(num_classes):\n            inds = (classes == j)\n            # x, y, z, h, w, l, yaw\n            top_preds[j] = np.concatenate([\n                detections[i, inds, 0:1],\n                detections[i, inds, 1:2] * down_ratio,\n                detections[i, inds, 2:3] * down_ratio,\n                detections[i, inds, 3:4],\n                detections[i, inds, 4:5],\n                detections[i, inds, 5:6] / bound_size_y * BEV_WIDTH,\n                detections[i, inds, 6:7] / bound_size_x * BEV_HEIGHT,\n                get_yaw(detections[i, inds, 7:9]).astype(np.float32)], axis=1)\n            # Filter by peak_thresh\n            if len(top_preds[j]) > 0:\n                keep_inds = (top_preds[j][:, 0] > peak_thresh)\n                top_preds[j] = top_preds[j][keep_inds]\n        ret.append(top_preds)\n\n    return ret\n\n\ndef draw_predictions(img, detections, num_classes=3):\n    for j in range(num_classes):\n        if len(detections[j]) > 0:\n            for det in detections[j]:\n                # (scores-0:1, x-1:2, y-2:3, z-3:4, dim-4:7, yaw-7:8)\n                _score, _x, _y, _z, _h, _w, _l, _yaw = det\n                drawRotatedBox(img, _x, _y, _w, _l, _yaw, colors[int(j)])\n\n    return img\n\n\ndef convert_det_to_real_values(detections, num_classes=3):\n    kitti_dets = []\n    for cls_id in range(num_classes):\n        if len(detections[cls_id]) > 0:\n            for det in detections[cls_id]:\n                # (scores-0:1, x-1:2, y-2:3, z-3:4, dim-4:7, yaw-7:8)\n                _score, _x, _y, _z, _h, _w, _l, _yaw = det\n                _yaw = -_yaw\n                x = _y / BEV_HEIGHT * bound_size_x + boundary['minX']\n                y = _x / BEV_WIDTH * bound_size_y + boundary['minY']\n                z = _z + boundary['minZ']\n                w = _w / BEV_WIDTH * bound_size_y\n                l = _l / BEV_HEIGHT * bound_size_x\n\n                kitti_dets.append([cls_id, x, y, z, _h, w, l, _yaw])\n\n    return np.array(kitti_dets)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:44:52.179743Z","iopub.execute_input":"2021-12-13T06:44:52.180023Z","iopub.status.idle":"2021-12-13T06:44:52.216088Z","shell.execute_reply.started":"2021-12-13T06:44:52.179988Z","shell.execute_reply":"2021-12-13T06:44:52.215325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualization Utility Functions","metadata":{}},{"cell_type":"code","source":"from __future__ import print_function\nimport os\nimport sys\n\nimport numpy as np\nimport cv2\n\n\ndef roty(angle):\n    # Rotation about the y-axis.\n    c = np.cos(angle)\n    s = np.sin(angle)\n    return np.array([[c, 0, s],\n                     [0, 1, 0],\n                     [-s, 0, c]])\n\n\ndef compute_box_3d(dim, location, ry):\n    # dim: 3\n    # location: 3\n    # ry: 1\n    # return: 8 x 3\n    R = roty(ry)\n    h, w, l = dim\n    x_corners = [l / 2, l / 2, -l / 2, -l / 2, l / 2, l / 2, -l / 2, -l / 2]\n    y_corners = [0, 0, 0, 0, -h, -h, -h, -h]\n    z_corners = [w / 2, -w / 2, -w / 2, w / 2, w / 2, -w / 2, -w / 2, w / 2]\n\n    corners = np.array([x_corners, y_corners, z_corners], dtype=np.float32)\n    corners_3d = np.dot(R, corners)\n    corners_3d = corners_3d + np.array(location, dtype=np.float32).reshape(3, 1)\n    return corners_3d.transpose(1, 0)\n\n\ndef project_to_image(pts_3d, P):\n    # pts_3d: n x 3\n    # P: 3 x 4\n    # return: n x 2\n    pts_3d_homo = np.concatenate([pts_3d, np.ones((pts_3d.shape[0], 1), dtype=np.float32)], axis=1)\n    pts_2d = np.dot(P, pts_3d_homo.transpose(1, 0)).transpose(1, 0)\n    pts_2d = pts_2d[:, :2] / pts_2d[:, 2:]\n\n    return pts_2d.astype(np.int)\n\n\ndef draw_box_3d_v2(image, qs, color=(255, 0, 255), thickness=2):\n    ''' Draw 3d bounding box in image\n        qs: (8,3) array of vertices for the 3d box in following order:\n            1 -------- 0\n           /|         /|\n          2 -------- 3 .\n          | |        | |\n          . 5 -------- 4\n          |/         |/\n          6 -------- 7\n    '''\n    qs = qs.astype(np.int32)\n    for k in range(0, 4):\n        # Ref: http://docs.enthought.com/mayavi/mayavi/auto/mlab_helper_functions.html\n        i, j = k, (k + 1) % 4\n        # use LINE_AA for opencv3\n        cv2.line(image, (qs[i, 0], qs[i, 1]), (qs[j, 0], qs[j, 1]), color, thickness)\n\n        i, j = k + 4, (k + 1) % 4 + 4\n        cv2.line(image, (qs[i, 0], qs[i, 1]), (qs[j, 0], qs[j, 1]), color, thickness)\n\n        i, j = k, k + 4\n        cv2.line(image, (qs[i, 0], qs[i, 1]), (qs[j, 0], qs[j, 1]), color, thickness)\n\n    return image\n\n\ndef draw_box_3d(image, corners, color=(0, 0, 255)):\n    ''' Draw 3d bounding box in image\n        corners: (8,3) array of vertices for the 3d box in following order:\n            1 -------- 0\n           /|         /|\n          2 -------- 3 .\n          | |        | |\n          . 5 -------- 4\n          |/         |/\n          6 -------- 7\n    '''\n\n    face_idx = [[0, 1, 5, 4],\n                [1, 2, 6, 5],\n                [2, 3, 7, 6],\n                [3, 0, 4, 7]]\n    for ind_f in range(3, -1, -1):\n        f = face_idx[ind_f]\n        for j in range(4):\n            cv2.line(image, (corners[f[j], 0], corners[f[j], 1]),\n                     (corners[f[(j + 1) % 4], 0], corners[f[(j + 1) % 4], 1]), color, 2, lineType=cv2.LINE_AA)\n        if ind_f == 0:\n            cv2.line(image, (corners[f[0], 0], corners[f[0], 1]),\n                     (corners[f[2], 0], corners[f[2], 1]), color, 1, lineType=cv2.LINE_AA)\n            cv2.line(image, (corners[f[1], 0], corners[f[1], 1]),\n                     (corners[f[3], 0], corners[f[3], 1]), color, 1, lineType=cv2.LINE_AA)\n\n    return image\n\n\ndef show_rgb_image_with_boxes(img, labels, calib):\n    for box_idx, label in enumerate(labels):\n        cls_id, location, dim, ry = label[0], label[1:4], label[4:7], label[7]\n        if location[2] < 2.0:  # The object is too close to the camera, ignore it during visualization\n            continue\n        if cls_id < 0:\n            continue\n        corners_3d = compute_box_3d(dim, location, ry)\n        corners_2d = project_to_image(corners_3d, calib.P2)\n        img = draw_box_3d(img, corners_2d, color=colors[int(cls_id)])\n\n    return img\n\n\ndef merge_rgb_to_bev(img_rgb, img_bev, output_width):\n    img_rgb_h, img_rgb_w = img_rgb.shape[:2]\n    ratio_rgb = output_width / img_rgb_w\n    output_rgb_h = int(ratio_rgb * img_rgb_h)\n    ret_img_rgb = cv2.resize(img_rgb, (output_width, output_rgb_h))\n\n    img_bev_h, img_bev_w = img_bev.shape[:2]\n    ratio_bev = output_width / img_bev_w\n    output_bev_h = int(ratio_bev * img_bev_h)\n\n    ret_img_bev = cv2.resize(img_bev, (output_width, output_bev_h))\n\n    out_img = np.zeros((output_rgb_h + output_bev_h, output_width, 3), dtype=np.uint8)\n    # Upper: RGB --> BEV\n    out_img[:output_rgb_h, ...] = ret_img_rgb\n    out_img[output_rgb_h:, ...] = ret_img_bev\n\n    return out_img","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:44:52.219495Z","iopub.execute_input":"2021-12-13T06:44:52.219683Z","iopub.status.idle":"2021-12-13T06:44:52.249243Z","shell.execute_reply.started":"2021-12-13T06:44:52.21966Z","shell.execute_reply":"2021-12-13T06:44:52.24856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Demo Utility Functions","metadata":{}},{"cell_type":"code","source":"import sys\nimport os\nfrom builtins import int\nfrom glob import glob\n\nimport numpy as np\nfrom torch.utils.data import Dataset\nimport cv2\nimport torch\n\n\nclass Demo_KittiDataset(Dataset):\n    def __init__(self, configs):\n        self.dataset_dir = os.path.join(configs.dataset_dir, configs.foldername, configs.foldername[:10],\n                                        configs.foldername)\n        self.input_size = configs.input_size\n        self.hm_size = configs.hm_size\n\n        self.num_classes = configs.num_classes\n        self.max_objects = configs.max_objects\n\n        self.image_dir = os.path.join(self.dataset_dir, \"image_02\", \"data\")\n        self.lidar_dir = os.path.join(self.dataset_dir, \"velodyne_points\", \"data\")\n        self.label_dir = os.path.join(self.dataset_dir, \"label_2\", \"data\")\n        self.sample_id_list = sorted(glob(os.path.join(self.lidar_dir, '*.bin')))\n        self.sample_id_list = [float(os.path.basename(fn)[:-4]) for fn in self.sample_id_list]\n        self.num_samples = len(self.sample_id_list)\n\n    def __len__(self):\n        return len(self.sample_id_list)\n\n    def __getitem__(self, index):\n        pass\n\n    def load_bevmap_front(self, index):\n        \"\"\"Load only image for the testing phase\"\"\"\n        sample_id = int(self.sample_id_list[index])\n        img_path, img_rgb = self.get_image(sample_id)\n        lidarData = self.get_lidar(sample_id)\n        front_lidar = get_filtered_lidar(lidarData, boundary)\n        front_bevmap = makeBEVMap(front_lidar, boundary)\n        front_bevmap = torch.from_numpy(front_bevmap)\n\n        metadatas = {\n            'img_path': img_path,\n        }\n\n        return metadatas, front_bevmap, img_rgb\n\n    def load_bevmap_front_vs_back(self, index):\n        \"\"\"Load only image for the testing phase\"\"\"\n        sample_id = int(self.sample_id_list[index])\n        img_path, img_rgb = self.get_image(sample_id)\n        lidarData = self.get_lidar(sample_id)\n\n        front_lidar = get_filtered_lidar(lidarData, boundary)\n        front_bevmap = makeBEVMap(front_lidar, boundary)\n        front_bevmap = torch.from_numpy(front_bevmap)\n\n        back_lidar = get_filtered_lidar(lidarData, boundary_back)\n        back_bevmap = makeBEVMap(back_lidar, boundary_back)\n        back_bevmap = torch.from_numpy(back_bevmap)\n\n        metadatas = {\n            'img_path': img_path,\n        }\n\n        return metadatas, front_bevmap, back_bevmap, img_rgb\n\n    def get_image(self, idx):\n        img_path = os.path.join(self.image_dir, '{:010d}.png'.format(idx))\n        img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n\n        return img_path, img\n\n    def get_lidar(self, idx):\n        lidar_file = os.path.join(self.lidar_dir, '{:010d}.bin'.format(idx))\n        # assert os.path.isfile(lidar_file)\n        return np.fromfile(lidar_file, dtype=np.float32).reshape(-1, 4)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:44:52.250743Z","iopub.execute_input":"2021-12-13T06:44:52.251152Z","iopub.status.idle":"2021-12-13T06:44:52.2691Z","shell.execute_reply.started":"2021-12-13T06:44:52.251115Z","shell.execute_reply":"2021-12-13T06:44:52.268333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install wget\n!mkdir -p ./dataset/kitti/demo\n!mkdir -p ./dataset/kitti/ImageSets\n\n!wget https://raw.githubusercontent.com/maudzung/SFA3D/master/dataset/kitti/demo/calib.txt -P ./dataset/kitti/demo/\n!wget https://raw.githubusercontent.com/maudzung/SFA3D/master/dataset/kitti/ImageSets/test.txt -P ./dataset/kitti/ImageSets/\n!wget https://raw.githubusercontent.com/maudzung/SFA3D/master/dataset/kitti/ImageSets/train.txt -P ./dataset/kitti/ImageSets/\n!wget https://raw.githubusercontent.com/maudzung/SFA3D/master/dataset/kitti/ImageSets/trainval.txt -P ./dataset/kitti/ImageSets/\n!wget https://raw.githubusercontent.com/maudzung/SFA3D/master/dataset/kitti/ImageSets/val.txt -P ./dataset/kitti/ImageSets/","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:44:52.270774Z","iopub.execute_input":"2021-12-13T06:44:52.271539Z","iopub.status.idle":"2021-12-13T06:45:05.178865Z","shell.execute_reply.started":"2021-12-13T06:44:52.271491Z","shell.execute_reply":"2021-12-13T06:45:05.178027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import argparse\nimport sys\nimport os\nimport warnings\nimport zipfile\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nfrom easydict import EasyDict as edict\nimport numpy as np\nimport wget\nimport torch\nimport cv2\n\n\ndef parse_demo_configs():\n    config_dict = {}\n    config_dict['saved_fn'] = 'fpn_resnet_18'\n    config_dict['arch'] = 'fpn_resnet_18'\n    config_dict['pretrained_path'] = './checkpoints/fpn_resnet_18/fpn_resnet_18_epoch_300.pth'\n    config_dict['foldername'] = '2011_09_26_drive_0014_sync'\n    config_dict['K'] = 50\n    config_dict['gpu_idx'] = 0\n    config_dict['peak_thresh'] = 0.2\n    config_dict['output_width'] = 608\n    config_dict['no_cuda'] = False\n\n    configs = edict(config_dict)\n    configs.pin_memory = True\n    configs.distributed = False  # For testing on 1 GPU only\n\n    configs.input_size = (608, 608)\n    configs.hm_size = (152, 152)\n    configs.down_ratio = 4\n    configs.max_objects = 50\n\n    configs.imagenet_pretrained = False\n    configs.head_conv = 64\n    configs.num_classes = 3\n    configs.num_center_offset = 2\n    configs.num_z = 1\n    configs.num_dim = 3\n    configs.num_direction = 2  # sin, cos\n\n    configs.heads = {\n        'hm_cen': configs.num_classes,\n        'cen_offset': configs.num_center_offset,\n        'direction': configs.num_direction,\n        'z_coor': configs.num_z,\n        'dim': configs.num_dim\n    }\n\n    ####################################################################\n    ##############Dataset, Checkpoints, and results dir configs#########\n    ####################################################################\n    configs.root_dir = './'\n    configs.dataset_dir = os.path.join(configs.root_dir, 'dataset', 'kitti', 'demo')\n    configs.calib_path = os.path.join(configs.root_dir, 'dataset', 'kitti', 'demo', 'calib.txt')\n    configs.results_dir = os.path.join(configs.root_dir, 'results', configs.saved_fn)\n    make_folder(configs.results_dir)\n\n    return configs\n\n\ndef download_and_unzip(demo_dataset_dir, download_url):\n    filename = download_url.split('/')[-1]\n    filepath = os.path.join(demo_dataset_dir, filename)\n    if os.path.isfile(filepath):\n        print('The dataset have been downloaded')\n        return\n    print('\\nDownloading data for demonstration...')\n    wget.download(download_url, filepath)\n    print('\\nUnzipping the downloaded data...')\n    with zipfile.ZipFile(filepath, \"r\") as zip_ref:\n        zip_ref.extractall(os.path.join(demo_dataset_dir, filename[:-4]))\n\n\ndef do_detect(configs, model, bevmap, is_front):\n    if not is_front:\n        bevmap = torch.flip(bevmap, [1, 2])\n\n    input_bev_maps = bevmap.unsqueeze(0).to(configs.device, non_blocking=True).float()\n    t1 = time_synchronized()\n    outputs = model(input_bev_maps)\n    outputs['hm_cen'] = _sigmoid(outputs['hm_cen'])\n    outputs['cen_offset'] = _sigmoid(outputs['cen_offset'])\n    # detections size (batch_size, K, 10)\n    detections = decode(outputs['hm_cen'], outputs['cen_offset'], outputs['direction'], outputs['z_coor'],\n                        outputs['dim'], K=configs.K)\n    detections = detections.cpu().numpy().astype(np.float32)\n    detections = post_processing(detections, configs.num_classes, configs.down_ratio, configs.peak_thresh)\n    t2 = time_synchronized()\n    # Inference speed\n    fps = 1 / (t2 - t1)\n\n    return detections[0], bevmap, fps\n\n\ndef write_credit(img, org_author=(500, 400), text_author='github.com/maudzung', org_fps=(50, 1000), fps=None):\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    fontScale = 1\n    color = (255, 255, 255)\n    thickness = 2\n\n    cv2.putText(img, text_author, org_author, font, fontScale, color, thickness, cv2.LINE_AA)\n    cv2.putText(img, 'Speed: {:.1f} FPS'.format(fps), org_fps, font, fontScale, color, thickness, cv2.LINE_AA)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:45:05.182325Z","iopub.execute_input":"2021-12-13T06:45:05.182545Z","iopub.status.idle":"2021-12-13T06:45:05.20463Z","shell.execute_reply.started":"2021-12-13T06:45:05.182517Z","shell.execute_reply":"2021-12-13T06:45:05.203381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss Functions","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport math\n\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\n\n\ndef _gather_feat(feat, ind, mask=None):\n    dim = feat.size(2)\n    ind = ind.unsqueeze(2).expand(ind.size(0), ind.size(1), dim)\n    feat = feat.gather(1, ind)\n    if mask is not None:\n        mask = mask.unsqueeze(2).expand_as(feat)\n        feat = feat[mask]\n        feat = feat.view(-1, dim)\n    return feat\n\n\ndef _transpose_and_gather_feat(feat, ind):\n    feat = feat.permute(0, 2, 3, 1).contiguous()\n    feat = feat.view(feat.size(0), -1, feat.size(3))\n    feat = _gather_feat(feat, ind)\n    return feat\n\n\ndef _neg_loss(pred, gt, alpha=2, beta=4):\n    ''' Modified focal loss. Exactly the same as CornerNet.\n        Runs faster and costs a little bit more memory\n      Arguments:\n        pred (batch x c x h x w)\n        gt_regr (batch x c x h x w)\n    '''\n    pos_inds = gt.eq(1).float()\n    neg_inds = gt.lt(1).float()\n\n    neg_weights = torch.pow(1 - gt, beta)\n\n    loss = 0\n\n    pos_loss = torch.log(pred) * torch.pow(1 - pred, alpha) * pos_inds\n    neg_loss = torch.log(1 - pred) * torch.pow(pred, alpha) * neg_weights * neg_inds\n\n    num_pos = pos_inds.float().sum()\n    pos_loss = pos_loss.sum()\n    neg_loss = neg_loss.sum()\n\n    if num_pos == 0:\n        loss = loss - neg_loss\n    else:\n        loss = loss - (pos_loss + neg_loss) / num_pos\n    return loss\n\n\nclass FocalLoss(nn.Module):\n    '''nn.Module warpper for focal loss'''\n\n    def __init__(self):\n        super(FocalLoss, self).__init__()\n        self.neg_loss = _neg_loss\n\n    def forward(self, out, target):\n        return self.neg_loss(out, target)\n\n\nclass L1Loss(nn.Module):\n    def __init__(self):\n        super(L1Loss, self).__init__()\n\n    def forward(self, output, mask, ind, target):\n        pred = _transpose_and_gather_feat(output, ind)\n        mask = mask.unsqueeze(2).expand_as(pred).float()\n        loss = F.l1_loss(pred * mask, target * mask, size_average=False)\n        loss = loss / (mask.sum() + 1e-4)\n        return loss\n\n\nclass L1Loss_Balanced(nn.Module):\n    \"\"\"Balanced L1 Loss\n    paper: https://arxiv.org/pdf/1904.02701.pdf (CVPR 2019)\n    Code refer from: https://github.com/OceanPang/Libra_R-CNN\n    \"\"\"\n\n    def __init__(self, alpha=0.5, gamma=1.5, beta=1.0):\n        super(L1Loss_Balanced, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        assert beta > 0\n        self.beta = beta\n\n    def forward(self, output, mask, ind, target):\n        pred = _transpose_and_gather_feat(output, ind)\n        mask = mask.unsqueeze(2).expand_as(pred).float()\n        loss = self.balanced_l1_loss(pred * mask, target * mask)\n        loss = loss.sum() / (mask.sum() + 1e-4)\n\n        return loss\n\n    def balanced_l1_loss(self, pred, target):\n        assert pred.size() == target.size() and target.numel() > 0\n\n        diff = torch.abs(pred - target)\n        b = math.exp(self.gamma / self.alpha) - 1\n        loss = torch.where(diff < self.beta,\n                           self.alpha / b * (b * diff + 1) * torch.log(b * diff / self.beta + 1) - self.alpha * diff,\n                           self.gamma * diff + self.gamma / b - self.alpha * self.beta)\n\n        return loss\n\n\nclass Compute_Loss(nn.Module):\n    def __init__(self, device):\n        super(Compute_Loss, self).__init__()\n        self.device = device\n        self.focal_loss = FocalLoss()\n        self.l1_loss = L1Loss()\n        self.l1_loss_balanced = L1Loss_Balanced(alpha=0.5, gamma=1.5, beta=1.0)\n        self.weight_hm_cen = 1.\n        self.weight_z_coor, self.weight_cenoff, self.weight_dim, self.weight_direction = 1., 1., 1., 1.\n\n    def forward(self, outputs, tg):\n        # tg: targets\n        outputs['hm_cen'] = _sigmoid(outputs['hm_cen'])\n        outputs['cen_offset'] = _sigmoid(outputs['cen_offset'])\n\n        l_hm_cen = self.focal_loss(outputs['hm_cen'], tg['hm_cen'])\n        l_cen_offset = self.l1_loss(outputs['cen_offset'], tg['obj_mask'], tg['indices_center'], tg['cen_offset'])\n        l_direction = self.l1_loss(outputs['direction'], tg['obj_mask'], tg['indices_center'], tg['direction'])\n        # Apply the L1_loss balanced for z coor and dimension regression\n        l_z_coor = self.l1_loss_balanced(outputs['z_coor'], tg['obj_mask'], tg['indices_center'], tg['z_coor'])\n        l_dim = self.l1_loss_balanced(outputs['dim'], tg['obj_mask'], tg['indices_center'], tg['dim'])\n\n        total_loss = l_hm_cen * self.weight_hm_cen + l_cen_offset * self.weight_cenoff + \\\n                     l_dim * self.weight_dim + l_direction * self.weight_direction + \\\n                     l_z_coor * self.weight_z_coor\n\n        loss_stats = {\n            'total_loss': to_cpu(total_loss).item(),\n            'hm_cen_loss': to_cpu(l_hm_cen).item(),\n            'cen_offset_loss': to_cpu(l_cen_offset).item(),\n            'dim_loss': to_cpu(l_dim).item(),\n            'direction_loss': to_cpu(l_direction).item(),\n            'z_coor_loss': to_cpu(l_z_coor).item(),\n        }\n\n        return total_loss, loss_stats","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:45:05.206399Z","iopub.execute_input":"2021-12-13T06:45:05.206908Z","iopub.status.idle":"2021-12-13T06:45:05.237095Z","shell.execute_reply.started":"2021-12-13T06:45:05.206812Z","shell.execute_reply":"2021-12-13T06:45:05.236319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"markdown","source":"## FPN ResNet","metadata":{}},{"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\nimport torch.nn.functional as F\n\nBN_MOMENTUM = 0.1\n\nmodel_urls = {\n    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion, momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass PoseResNet(nn.Module):\n\n    def __init__(self, block, layers, heads, head_conv, **kwargs):\n        self.inplanes = 64\n        self.deconv_with_bias = False\n        self.heads = heads\n\n        super(PoseResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64, momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n\n        self.conv_up_level1 = nn.Conv2d(768, 256, kernel_size=1, stride=1, padding=0)\n        self.conv_up_level2 = nn.Conv2d(384, 128, kernel_size=1, stride=1, padding=0)\n        self.conv_up_level3 = nn.Conv2d(192, 64, kernel_size=1, stride=1, padding=0)\n\n        fpn_channels = [256, 128, 64]\n        for fpn_idx, fpn_c in enumerate(fpn_channels):\n            for head in sorted(self.heads):\n                num_output = self.heads[head]\n                if head_conv > 0:\n                    fc = nn.Sequential(\n                        nn.Conv2d(fpn_c, head_conv, kernel_size=3, padding=1, bias=True),\n                        nn.ReLU(inplace=True),\n                        nn.Conv2d(head_conv, num_output, kernel_size=1, stride=1, padding=0))\n                else:\n                    fc = nn.Conv2d(in_channels=fpn_c, out_channels=num_output, kernel_size=1, stride=1, padding=0)\n\n                self.__setattr__('fpn{}_{}'.format(fpn_idx, head), fc)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        _, _, input_h, input_w = x.size()\n        hm_h, hm_w = input_h // 4, input_w // 4\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        out_layer1 = self.layer1(x)\n        out_layer2 = self.layer2(out_layer1)\n\n        out_layer3 = self.layer3(out_layer2)\n\n        out_layer4 = self.layer4(out_layer3)\n\n        # up_level1: torch.Size([b, 512, 14, 14])\n        up_level1 = F.interpolate(out_layer4, scale_factor=2, mode='bilinear', align_corners=True)\n\n        concat_level1 = torch.cat((up_level1, out_layer3), dim=1)\n        # up_level2: torch.Size([b, 256, 28, 28])\n        up_level2 = F.interpolate(self.conv_up_level1(concat_level1), scale_factor=2, mode='bilinear',\n                                  align_corners=True)\n\n        concat_level2 = torch.cat((up_level2, out_layer2), dim=1)\n        # up_level3: torch.Size([b, 128, 56, 56]),\n        up_level3 = F.interpolate(self.conv_up_level2(concat_level2), scale_factor=2, mode='bilinear',\n                                  align_corners=True)\n        # up_level4: torch.Size([b, 64, 56, 56])\n        up_level4 = self.conv_up_level3(torch.cat((up_level3, out_layer1), dim=1))\n\n        ret = {}\n        for head in self.heads:\n            temp_outs = []\n            for fpn_idx, fdn_input in enumerate([up_level2, up_level3, up_level4]):\n                fpn_out = self.__getattr__('fpn{}_{}'.format(fpn_idx, head))(fdn_input)\n                _, _, fpn_out_h, fpn_out_w = fpn_out.size()\n                # Make sure the added features having same size of heatmap output\n                if (fpn_out_w != hm_w) or (fpn_out_h != hm_h):\n                    fpn_out = F.interpolate(fpn_out, size=(hm_h, hm_w))\n                temp_outs.append(fpn_out)\n            # Take the softmax in the keypoint feature pyramid network\n            final_out = self.apply_kfpn(temp_outs)\n\n            ret[head] = final_out\n\n        return ret\n\n    def apply_kfpn(self, outs):\n        outs = torch.cat([out.unsqueeze(-1) for out in outs], dim=-1)\n        softmax_outs = F.softmax(outs, dim=-1)\n        ret_outs = (outs * softmax_outs).sum(dim=-1)\n        return ret_outs\n\n    def init_weights(self, num_layers, pretrained=True):\n        if pretrained:\n            # TODO: Check initial weights for head later\n            for fpn_idx in [0, 1, 2]:  # 3 FPN layers\n                for head in self.heads:\n                    final_layer = self.__getattr__('fpn{}_{}'.format(fpn_idx, head))\n                    for i, m in enumerate(final_layer.modules()):\n                        if isinstance(m, nn.Conv2d):\n                            # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                            # print('=> init {}.weight as normal(0, 0.001)'.format(name))\n                            # print('=> init {}.bias as 0'.format(name))\n                            if m.weight.shape[0] == self.heads[head]:\n                                if 'hm' in head:\n                                    nn.init.constant_(m.bias, -2.19)\n                                else:\n                                    nn.init.normal_(m.weight, std=0.001)\n                                    nn.init.constant_(m.bias, 0)\n            # pretrained_state_dict = torch.load(pretrained)\n            url = model_urls['resnet{}'.format(num_layers)]\n            pretrained_state_dict = model_zoo.load_url(url)\n            print('=> loading pretrained model {}'.format(url))\n            self.load_state_dict(pretrained_state_dict, strict=False)\n\n\nresnet_spec = {18: (BasicBlock, [2, 2, 2, 2]),\n               34: (BasicBlock, [3, 4, 6, 3]),\n               50: (Bottleneck, [3, 4, 6, 3]),\n               101: (Bottleneck, [3, 4, 23, 3]),\n               152: (Bottleneck, [3, 8, 36, 3])}\n\n\ndef get_pose_net(num_layers, heads, head_conv, imagenet_pretrained):\n    block_class, layers = resnet_spec[num_layers]\n\n    model = PoseResNet(block_class, layers, heads, head_conv=head_conv)\n    model.init_weights(num_layers, pretrained=imagenet_pretrained)\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:45:05.238565Z","iopub.execute_input":"2021-12-13T06:45:05.238982Z","iopub.status.idle":"2021-12-13T06:45:05.286119Z","shell.execute_reply.started":"2021-12-13T06:45:05.238949Z","shell.execute_reply":"2021-12-13T06:45:05.285449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Utility Functions","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\n\nimport torch\n\n\ndef create_model(configs):\n    \"\"\"Create model based on architecture name\"\"\"\n    try:\n        arch_parts = configs.arch.split('_')\n        num_layers = int(arch_parts[-1])\n    except:\n        raise ValueError\n    if 'fpn_resnet' in configs.arch:\n        print('using ResNet architecture with feature pyramid')\n        model = get_pose_net(num_layers=num_layers, heads=configs.heads, head_conv=configs.head_conv,\n                             imagenet_pretrained=configs.imagenet_pretrained)\n    else:\n        assert False, 'Undefined model backbone'\n\n    return model\n\n\ndef get_num_parameters(model):\n    \"\"\"Count number of trained parameters of the model\"\"\"\n    if hasattr(model, 'module'):\n        num_parameters = sum(p.numel() for p in model.module.parameters() if p.requires_grad)\n    else:\n        num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n    return num_parameters\n\n\ndef make_data_parallel(model, configs):\n    if configs.distributed:\n        # For multiprocessing distributed, DistributedDataParallel constructor\n        # should always set the single device scope, otherwise,\n        # DistributedDataParallel will use all available devices.\n        if configs.gpu_idx is not None:\n            torch.cuda.set_device(configs.gpu_idx)\n            model.cuda(configs.gpu_idx)\n            # When using a single GPU per process and per\n            # DistributedDataParallel, we need to divide the batch size\n            # ourselves based on the total number of GPUs we have\n            configs.batch_size = int(configs.batch_size / configs.ngpus_per_node)\n            configs.num_workers = int((configs.num_workers + configs.ngpus_per_node - 1) / configs.ngpus_per_node)\n            model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[configs.gpu_idx])\n        else:\n            model.cuda()\n            # DistributedDataParallel will divide and allocate batch_size to all\n            # available GPUs if device_ids are not set\n            model = torch.nn.parallel.DistributedDataParallel(model)\n    elif configs.gpu_idx is not None:\n        torch.cuda.set_device(configs.gpu_idx)\n        model = model.cuda(configs.gpu_idx)\n    else:\n        # DataParallel will divide and allocate batch_size to all available GPUs\n        model = torch.nn.DataParallel(model).cuda()\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:45:05.28754Z","iopub.execute_input":"2021-12-13T06:45:05.287874Z","iopub.status.idle":"2021-12-13T06:45:05.301569Z","shell.execute_reply.started":"2021-12-13T06:45:05.287838Z","shell.execute_reply":"2021-12-13T06:45:05.300674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"import time\nimport numpy as np\nimport sys\nimport random\nimport os\nimport warnings\n\nimport torch\nfrom torch.utils.tensorboard import SummaryWriter\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nimport torch.utils.data.distributed\nfrom tqdm import tqdm\n\n\ndef main():\n    configs = parse_configs()\n\n    # Re-produce results\n    if configs.seed is not None:\n        random.seed(configs.seed)\n        np.random.seed(configs.seed)\n        torch.manual_seed(configs.seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\n    if configs.gpu_idx is not None:\n        print('You have chosen a specific GPU. This will completely disable data parallelism.')\n\n    if configs.dist_url == \"env://\" and configs.world_size == -1:\n        configs.world_size = int(os.environ[\"WORLD_SIZE\"])\n\n    configs.distributed = configs.world_size > 1 or configs.multiprocessing_distributed\n\n    if configs.multiprocessing_distributed:\n        configs.world_size = configs.ngpus_per_node * configs.world_size\n        mp.spawn(main_worker, nprocs=configs.ngpus_per_node, args=(configs,))\n    else:\n        main_worker(configs.gpu_idx, configs)\n\n\ndef main_worker(gpu_idx, configs):\n    configs.gpu_idx = gpu_idx\n    configs.device = torch.device('cpu' if configs.gpu_idx is None else 'cuda:{}'.format(configs.gpu_idx))\n\n    if configs.distributed:\n        if configs.dist_url == \"env://\" and configs.rank == -1:\n            configs.rank = int(os.environ[\"RANK\"])\n        if configs.multiprocessing_distributed:\n            # For multiprocessing distributed training, rank needs to be the\n            # global rank among all the processes\n            configs.rank = configs.rank * configs.ngpus_per_node + gpu_idx\n\n        dist.init_process_group(backend=configs.dist_backend, init_method=configs.dist_url,\n                                world_size=configs.world_size, rank=configs.rank)\n        configs.subdivisions = int(64 / configs.batch_size / configs.ngpus_per_node)\n    else:\n        configs.subdivisions = int(64 / configs.batch_size)\n\n    configs.is_master_node = (not configs.distributed) or (\n            configs.distributed and (configs.rank % configs.ngpus_per_node == 0))\n\n    if configs.is_master_node:\n        logger = Logger(configs.logs_dir, configs.saved_fn)\n        logger.info('>>> Created a new logger')\n        logger.info('>>> configs: {}'.format(configs))\n        tb_writer = SummaryWriter(log_dir=os.path.join(configs.logs_dir, 'tensorboard'))\n    else:\n        logger = None\n        tb_writer = None\n\n    # model\n    model = create_model(configs)\n\n    # load weight from a checkpoint\n    if configs.pretrained_path is not None:\n        assert os.path.isfile(configs.pretrained_path), \"=> no checkpoint found at '{}'\".format(configs.pretrained_path)\n        model.load_state_dict(torch.load(configs.pretrained_path, map_location='cpu'))\n        if logger is not None:\n            logger.info('loaded pretrained model at {}'.format(configs.pretrained_path))\n\n    # resume weights of model from a checkpoint\n    if configs.resume_path is not None:\n        assert os.path.isfile(configs.resume_path), \"=> no checkpoint found at '{}'\".format(configs.resume_path)\n        model.load_state_dict(torch.load(configs.resume_path, map_location='cpu'))\n        if logger is not None:\n            logger.info('resume training model from checkpoint {}'.format(configs.resume_path))\n\n    # Data Parallel\n    model = make_data_parallel(model, configs)\n\n    # Make sure to create optimizer after moving the model to cuda\n    optimizer = create_optimizer(configs, model)\n    lr_scheduler = create_lr_scheduler(optimizer, configs)\n    configs.step_lr_in_epoch = False if configs.lr_type in ['multi_step', 'cosin', 'one_cycle'] else True\n\n    # resume optimizer, lr_scheduler from a checkpoint\n    if configs.resume_path is not None:\n        utils_path = configs.resume_path.replace('Model_', 'Utils_')\n        assert os.path.isfile(utils_path), \"=> no checkpoint found at '{}'\".format(utils_path)\n        utils_state_dict = torch.load(utils_path, map_location='cuda:{}'.format(configs.gpu_idx))\n        optimizer.load_state_dict(utils_state_dict['optimizer'])\n        lr_scheduler.load_state_dict(utils_state_dict['lr_scheduler'])\n        configs.start_epoch = utils_state_dict['epoch'] + 1\n\n    if configs.is_master_node:\n        num_parameters = get_num_parameters(model)\n        logger.info('number of trained parameters of the model: {}'.format(num_parameters))\n\n    if logger is not None:\n        logger.info(\">>> Loading dataset & getting dataloader...\")\n    # Create dataloader\n    train_dataloader, train_sampler = create_train_dataloader(configs)\n    if logger is not None:\n        logger.info('number of batches in training set: {}'.format(len(train_dataloader)))\n\n    if configs.evaluate:\n        val_dataloader = create_val_dataloader(configs)\n        val_loss = validate(val_dataloader, model, configs)\n        print('val_loss: {:.4e}'.format(val_loss))\n        return\n\n    for epoch in range(configs.start_epoch, configs.num_epochs + 1):\n        if logger is not None:\n            logger.info('{}'.format('*-' * 40))\n            logger.info('{} {}/{} {}'.format('=' * 35, epoch, configs.num_epochs, '=' * 35))\n            logger.info('{}'.format('*-' * 40))\n            logger.info('>>> Epoch: [{}/{}]'.format(epoch, configs.num_epochs))\n\n        if configs.distributed:\n            train_sampler.set_epoch(epoch)\n        # train for one epoch\n        train_one_epoch(train_dataloader, model, optimizer, lr_scheduler, epoch, configs, logger, tb_writer)\n        if (not configs.no_val) and (epoch % configs.checkpoint_freq == 0):\n            val_dataloader = create_val_dataloader(configs)\n            print('number of batches in val_dataloader: {}'.format(len(val_dataloader)))\n            val_loss = validate(val_dataloader, model, configs)\n            print('val_loss: {:.4e}'.format(val_loss))\n            if tb_writer is not None:\n                tb_writer.add_scalar('Val_loss', val_loss, epoch)\n\n        # Save checkpoint\n        if configs.is_master_node and ((epoch % configs.checkpoint_freq) == 0):\n            model_state_dict, utils_state_dict = get_saved_state(model, optimizer, lr_scheduler, epoch, configs)\n            save_checkpoint(configs.checkpoints_dir, configs.saved_fn, model_state_dict, utils_state_dict, epoch)\n\n        if not configs.step_lr_in_epoch:\n            lr_scheduler.step()\n            if tb_writer is not None:\n                tb_writer.add_scalar('LR', lr_scheduler.get_lr()[0], epoch)\n\n    if tb_writer is not None:\n        tb_writer.close()\n    if configs.distributed:\n        cleanup()\n\n\ndef cleanup():\n    dist.destroy_process_group()\n\n\ndef train_one_epoch(train_dataloader, model, optimizer, lr_scheduler, epoch, configs, logger, tb_writer):\n    batch_time = AverageMeter('Time', ':6.3f')\n    data_time = AverageMeter('Data', ':6.3f')\n    losses = AverageMeter('Loss', ':.4e')\n\n    progress = ProgressMeter(len(train_dataloader), [batch_time, data_time, losses],\n                             prefix=\"Train - Epoch: [{}/{}]\".format(epoch, configs.num_epochs))\n\n    criterion = Compute_Loss(device=configs.device)\n    num_iters_per_epoch = len(train_dataloader)\n    # switch to train mode\n    model.train()\n    start_time = time.time()\n    for batch_idx, batch_data in enumerate(tqdm(train_dataloader)):\n        data_time.update(time.time() - start_time)\n        metadatas, imgs, targets = batch_data\n        batch_size = imgs.size(0)\n        global_step = num_iters_per_epoch * (epoch - 1) + batch_idx + 1\n        for k in targets.keys():\n            targets[k] = targets[k].to(configs.device, non_blocking=True)\n        imgs = imgs.to(configs.device, non_blocking=True).float()\n        outputs = model(imgs)\n        total_loss, loss_stats = criterion(outputs, targets)\n        # For torch.nn.DataParallel case\n        if (not configs.distributed) and (configs.gpu_idx is None):\n            total_loss = torch.mean(total_loss)\n\n        # compute gradient and perform backpropagation\n        total_loss.backward()\n        if global_step % configs.subdivisions == 0:\n            optimizer.step()\n            # zero the parameter gradients\n            optimizer.zero_grad()\n            # Adjust learning rate\n            if configs.step_lr_in_epoch:\n                lr_scheduler.step()\n                if tb_writer is not None:\n                    tb_writer.add_scalar('LR', lr_scheduler.get_lr()[0], global_step)\n\n        if configs.distributed:\n            reduced_loss = reduce_tensor(total_loss.data, configs.world_size)\n        else:\n            reduced_loss = total_loss.data\n        losses.update(to_python_float(reduced_loss), batch_size)\n        # measure elapsed time\n        # torch.cuda.synchronize()\n        batch_time.update(time.time() - start_time)\n\n        if tb_writer is not None:\n            if (global_step % configs.tensorboard_freq) == 0:\n                loss_stats['avg_loss'] = losses.avg\n                tb_writer.add_scalars('Train', loss_stats, global_step)\n        # Log message\n        if logger is not None:\n            if (global_step % configs.print_freq) == 0:\n                logger.info(progress.get_message(batch_idx))\n\n        start_time = time.time()\n\n\ndef validate(val_dataloader, model, configs):\n    losses = AverageMeter('Loss', ':.4e')\n    criterion = Compute_Loss(device=configs.device)\n    # switch to train mode\n    model.eval()\n    with torch.no_grad():\n        for batch_idx, batch_data in enumerate(tqdm(val_dataloader)):\n            metadatas, imgs, targets = batch_data\n            batch_size = imgs.size(0)\n            for k in targets.keys():\n                targets[k] = targets[k].to(configs.device, non_blocking=True)\n            imgs = imgs.to(configs.device, non_blocking=True).float()\n            outputs = model(imgs)\n            total_loss, loss_stats = criterion(outputs, targets)\n            # For torch.nn.DataParallel case\n            if (not configs.distributed) and (configs.gpu_idx is None):\n                total_loss = torch.mean(total_loss)\n\n            if configs.distributed:\n                reduced_loss = reduce_tensor(total_loss.data, configs.world_size)\n            else:\n                reduced_loss = total_loss.data\n            losses.update(to_python_float(reduced_loss), batch_size)\n\n    return losses.avg","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:45:05.303025Z","iopub.execute_input":"2021-12-13T06:45:05.303445Z","iopub.status.idle":"2021-12-13T06:45:05.349575Z","shell.execute_reply.started":"2021-12-13T06:45:05.30341Z","shell.execute_reply":"2021-12-13T06:45:05.348945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Uncomment to start training\n# main()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:45:05.35083Z","iopub.execute_input":"2021-12-13T06:45:05.351143Z","iopub.status.idle":"2021-12-13T06:45:05.362865Z","shell.execute_reply.started":"2021-12-13T06:45:05.351101Z","shell.execute_reply":"2021-12-13T06:45:05.362094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Demo Testing","metadata":{}},{"cell_type":"code","source":"# Download weights\n!mkdir -p ./checkpoints/fpn_resnet_18\n!wget https://github.com/maudzung/SFA3D/raw/master/checkpoints/fpn_resnet_18/fpn_resnet_18_epoch_300.pth -P ./checkpoints/fpn_resnet_18/","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:45:05.364305Z","iopub.execute_input":"2021-12-13T06:45:05.369157Z","iopub.status.idle":"2021-12-13T06:45:07.739099Z","shell.execute_reply.started":"2021-12-13T06:45:05.369109Z","shell.execute_reply":"2021-12-13T06:45:07.7383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nimport os\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nimport cv2\nimport torch\nimport numpy as np\n\n\nconfigs = parse_demo_configs()\n\n# Try to download the dataset for demonstration\nserver_url = 'https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data'\ndownload_url = '{}/{}/{}.zip'.format(server_url, configs.foldername[:-5], configs.foldername)\ndownload_and_unzip(configs.dataset_dir, download_url)\n\nmodel = create_model(configs)\nprint('\\n\\n' + '-*=' * 30 + '\\n\\n')\nassert os.path.isfile(configs.pretrained_path), \"No file at {}\".format(configs.pretrained_path)\nmodel.load_state_dict(torch.load(configs.pretrained_path, map_location='cpu'))\nprint('Loaded weights from {}\\n'.format(configs.pretrained_path))\n\nconfigs.device = torch.device('cpu' if configs.no_cuda else 'cuda:{}'.format(configs.gpu_idx))\nmodel = model.to(device=configs.device)\nmodel.eval()\n\nout_cap = None\ndemo_dataset = Demo_KittiDataset(configs)\nwith torch.no_grad():\n    for sample_idx in range(len(demo_dataset)):\n        metadatas, front_bevmap, back_bevmap, img_rgb = demo_dataset.load_bevmap_front_vs_back(sample_idx)\n        front_detections, front_bevmap, fps = do_detect(configs, model, front_bevmap, is_front=True)\n        back_detections, back_bevmap, _ = do_detect(configs, model, back_bevmap, is_front=False)\n\n        # Draw prediction in the image\n        front_bevmap = (front_bevmap.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n        front_bevmap = cv2.resize(front_bevmap, (BEV_WIDTH, BEV_HEIGHT))\n        front_bevmap = draw_predictions(front_bevmap, front_detections, configs.num_classes)\n        # Rotate the front_bevmap\n        front_bevmap = cv2.rotate(front_bevmap, cv2.ROTATE_90_COUNTERCLOCKWISE)\n\n        # Draw prediction in the image\n        back_bevmap = (back_bevmap.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n        back_bevmap = cv2.resize(back_bevmap, (BEV_WIDTH, BEV_HEIGHT))\n        back_bevmap = draw_predictions(back_bevmap, back_detections, configs.num_classes)\n        # Rotate the back_bevmap\n        back_bevmap = cv2.rotate(back_bevmap, cv2.ROTATE_90_CLOCKWISE)\n\n        # merge front and back bevmap\n        full_bev = np.concatenate((back_bevmap, front_bevmap), axis=1)\n\n        img_path = metadatas['img_path'][0]\n        img_bgr = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)\n        calib = Calibration(configs.calib_path)\n        kitti_dets = convert_det_to_real_values(front_detections)\n        if len(kitti_dets) > 0:\n            kitti_dets[:, 1:] = lidar_to_camera_box(kitti_dets[:, 1:], calib.V2C, calib.R0, calib.P2)\n            img_bgr = show_rgb_image_with_boxes(img_bgr, kitti_dets, calib)\n        img_bgr = cv2.resize(img_bgr, (BEV_WIDTH * 2, 375))\n\n        out_img = np.concatenate((img_bgr, full_bev), axis=0)\n        write_credit(out_img, (50, 410), text_author='Cre: github.com/maudzung', org_fps=(900, 410), fps=fps)\n\n        if out_cap is None:\n            out_cap_h, out_cap_w = out_img.shape[:2]\n            fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n            out_path = os.path.join(configs.results_dir, '{}_both_2_sides.avi'.format(configs.foldername))\n            print('Create video writer at {}'.format(out_path))\n            out_cap = cv2.VideoWriter(out_path, fourcc, 5, (out_cap_w, out_cap_h))\n\n        out_cap.write(out_img)\n\nif out_cap:\n    out_cap.release()\n    \nprint(\"Video Created!\")","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:45:07.741084Z","iopub.execute_input":"2021-12-13T06:45:07.741409Z","iopub.status.idle":"2021-12-13T06:45:53.224483Z","shell.execute_reply.started":"2021-12-13T06:45:07.741368Z","shell.execute_reply":"2021-12-13T06:45:53.223649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}