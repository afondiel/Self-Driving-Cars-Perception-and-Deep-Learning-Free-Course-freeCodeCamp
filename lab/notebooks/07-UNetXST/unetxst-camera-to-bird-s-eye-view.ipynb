{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2936770,"sourceType":"datasetVersion","datasetId":1800545},{"sourceId":7930958,"sourceType":"datasetVersion","datasetId":1792887}],"dockerImageVersionId":30153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Cam2BEV using UNetXST\n\nGitHub: https://github.com/ika-rwth-aachen/Cam2BEV\n\nPaper: https://arxiv.org/abs/2005.04078","metadata":{}},{"cell_type":"markdown","source":"# Utility Functions","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport numpy as np\nimport random\nimport tensorflow as tf\nimport cv2\nfrom tqdm import tqdm\n\n\ndef get_files_in_folder(folder):\n    return sorted([os.path.join(folder, f) for f in os.listdir(folder)])\n\ndef sample_list(*ls, n_samples, replace=False):\n    n_samples = min(len(ls[0]), n_samples)\n    idcs = np.random.choice(np.arange(0, len(ls[0])), n_samples, replace=replace)\n    samples = zip([np.take(l, idcs) for l in ls])\n    return samples, idcs\n\ndef load_image(filename):\n    img = cv2.imread(filename)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    return img\n\ndef load_image_op(filename):\n    img = tf.io.read_file(filename)\n    img = tf.image.decode_png(img, channels=3)\n    return img\n\ndef resize_image(img, shape, interpolation=cv2.INTER_CUBIC):\n    # resize relevant image axis to length of corresponding target axis while preserving aspect ratio\n    axis = 0 if float(shape[0]) / float(img.shape[0]) > float(shape[1]) / float(img.shape[1]) else 1\n    factor = float(shape[axis]) / float(img.shape[axis])\n    img = cv2.resize(img, (0,0), fx=factor, fy=factor, interpolation=interpolation)\n\n    # crop other image axis to match target shape\n    center = img.shape[int(not axis)] / 2.0\n    step = shape[int(not axis)] / 2.0\n    left = int(center-step)\n    right = int(center+step)\n    if axis == 0:\n        img = img[:, left:right]\n    else:\n        img = img[left:right, :]\n\n    return img\n\ndef resize_image_op(img, fromShape, toShape, cropToPreserveAspectRatio=True, interpolation=tf.image.ResizeMethod.BICUBIC):\n    if not cropToPreserveAspectRatio:\n        img = tf.image.resize(img, toShape, method=interpolation)\n\n    else:\n        # first crop to match target aspect ratio\n        fx = toShape[1] / fromShape[1]\n        fy = toShape[0] / fromShape[0]\n        relevantAxis = 0 if fx < fy else 1\n        if relevantAxis == 0:\n            crop = fromShape[0] * toShape[1] / toShape[0]\n            img = tf.image.crop_to_bounding_box(img, 0, int((fromShape[1] - crop) / 2), fromShape[0], int(crop))\n        else:\n            crop = fromShape[1] * toShape[0] / toShape[1]\n            img = tf.image.crop_to_bounding_box(img, int((fromShape[0] - crop) / 2), 0, int(crop), fromShape[1])\n\n        # then resize to target shape\n        img = tf.image.resize(img, toShape, method=interpolation)\n\n    return img\n\n\ndef one_hot_encode_image(image, palette):\n    one_hot_map = []\n\n    # find instances of class colors and append layer to one-hot-map\n    for class_colors in palette:\n        class_map = np.zeros(image.shape[0:2], dtype=bool)\n        for color in class_colors:\n            class_map = class_map | (image == color).all(axis=-1)\n        one_hot_map.append(class_map)\n\n    # finalize one-hot-map\n    one_hot_map = np.stack(one_hot_map, axis=-1)\n    one_hot_map = one_hot_map.astype(np.float32)\n\n    return one_hot_map\n\n\ndef one_hot_encode_image_op(image, palette):\n    one_hot_map = []\n\n    for class_colors in palette:\n\n        class_map = tf.zeros(image.shape[0:2], dtype=tf.int32)\n\n        for color in class_colors:\n            # find instances of color and append layer to one-hot-map\n            class_map = tf.bitwise.bitwise_or(class_map, tf.cast(tf.reduce_all(tf.equal(image, color), axis=-1), tf.int32))\n        one_hot_map.append(class_map)\n\n    # finalize one-hot-map\n    one_hot_map = tf.stack(one_hot_map, axis=-1)\n    one_hot_map = tf.cast(one_hot_map, tf.float32)\n\n    return one_hot_map\n\n\ndef one_hot_decode_image(one_hot_image, palette):\n    # create empty image with correct dimensions\n    height, width = one_hot_image.shape[0:2]\n    depth = palette[0][0].size\n    image = np.zeros([height, width, depth])\n\n    # reduce all layers of one-hot-encoding to one layer with indices of the classes\n    map_of_classes = one_hot_image.argmax(2)\n\n    for idx, class_colors in enumerate(palette):\n        # fill image with corresponding class colors\n        image[np.where(map_of_classes == idx)] = class_colors[0]\n\n    image = image.astype(np.uint8)\n\n    return image\n\ndef get_class_distribution(folder, shape, palette):\n    # get filepaths\n    files = [os.path.join(folder, f) for f in os.listdir(folder) if not f.startswith(\".\")]\n\n    n_classes = len(palette)\n\n    def get_img(file, shape, interpolation=cv2.INTER_NEAREST, one_hot_reduce=False):\n        img = load_image(file)\n        img = resize_image(img, shape, interpolation)\n        img = one_hot_encode_image(img, palette)\n        return img\n\n    px = shape[0] * shape[1]\n\n    distribution = {}\n    for k in range(n_classes):\n        distribution[str(k)] = 0\n\n    i = 0\n    bar = tqdm(files)\n    for f in bar:\n        img = get_img(f, shape)\n        classes = np.argmax(img, axis=-1)\n        unique, counts = np.unique(classes, return_counts=True)\n        occs = dict(zip(unique, counts))\n\n        for k in range(n_classes):\n            occ = occs[k] if k in occs.keys() else 0\n            distribution[str(k)] = (distribution[str(k)] * i + occ / px) / (i+1)\n\n        bar.set_postfix(distribution)\n\n        i += 1\n\n    return distribution\n\n\ndef weighted_categorical_crossentropy(weights):\n    def wcce(y_true, y_pred):\n        Kweights = tf.constant(weights)\n        if not tf.is_tensor(y_pred): y_pred = tf.constant(y_pred)\n        y_true = tf.cast(y_true, y_pred.dtype)\n        return tf.keras.backend.categorical_crossentropy(y_true, y_pred) * tf.keras.backend.sum(y_true * Kweights, axis=-1)\n\n    return wcce\n\n\nclass MeanIoUWithOneHotLabels(tf.keras.metrics.MeanIoU):\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        y_true = tf.argmax(y_true, axis=-1)\n        y_pred = tf.argmax(y_pred, axis=-1)\n        return super().update_state(y_true, y_pred, sample_weight)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T05:22:02.175163Z","iopub.execute_input":"2021-12-17T05:22:02.175564Z","iopub.status.idle":"2021-12-17T05:22:08.485919Z","shell.execute_reply.started":"2021-12-17T05:22:02.17546Z","shell.execute_reply":"2021-12-17T05:22:08.485114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"from easydict import EasyDict as edict\n\nTRAIN_PATH = '../input/semantic-segmentation-bev/cam2bev-data-master-1_FRLR/1_FRLR/train/'\nVAL_PATH = '../input/semantic-segmentation-bev/cam2bev-data-master-1_FRLR/1_FRLR/val/'\nconfigs = {\n    \"input_training\": [TRAIN_PATH + 'front/front/', TRAIN_PATH + 'rear/rear/', TRAIN_PATH + 'left/left/', TRAIN_PATH + '/right/right/'],\n    \"label_training\": TRAIN_PATH + 'bev+occlusion/bev+occlusion/',\n    \"max_samples_training\": 10000,\n    \"input_validation\": [VAL_PATH + 'front/front/', VAL_PATH + 'rear/rear/', VAL_PATH + 'left/left/', VAL_PATH + '/right/right/'],\n    \"label_validation\": VAL_PATH + 'bev+occlusion/bev+occlusion/',\n    \"input_testing\": [VAL_PATH + 'front/front/', VAL_PATH + 'rear/rear/', VAL_PATH + 'left/left/', VAL_PATH + '/right/right/'],\n    \"label_testing\": VAL_PATH + 'bev+occlusion/bev+occlusion/',\n    \"max_samples_validation\": 10000,\n    \"max_samples_testing\": 10000,\n    \"image_shape\": [128, 256],\n    \"epochs\": 20,\n    \"batch_size\": 64,\n    \"learning_rate\": 1e-4,\n    \"loss_weights\": [0.98684351, 2.2481491, 10.47452063, 4.78351389, 7.01028204, 8.41360361, 10.91633349, 2.38571558, 1.02473193, 2.79359197],\n    \"early_stopping_patience\": 20,\n    \"save_interval\": 5,\n    \"output_dir\": \"output\"\n}\n\nconf = edict(configs)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T05:22:08.487461Z","iopub.execute_input":"2021-12-17T05:22:08.487679Z","iopub.status.idle":"2021-12-17T05:22:08.497753Z","shell.execute_reply.started":"2021-12-17T05:22:08.487654Z","shell.execute_reply":"2021-12-17T05:22:08.49678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Palette Configuration for identifying classes based on colors\nconf.one_hot_palette_input = np.array([\n    np.array([[128, 64, 128]]),\n    np.array([[244, 35, 232], [250, 170, 160]]),\n    np.array([[255, 0, 0]]),\n    np.array([[0, 0, 142], [0, 0, 110]]),\n    np.array([[0, 0, 70]]),\n    np.array([[0, 60, 100], [0, 0, 90]]),\n    np.array([[220, 20, 60], [0, 0, 230], [119, 11, 32]]),\n    \n    np.array([[0, 0, 0], [111, 74, 0], [81, 0, 81], [230, 150, 140], [70, 70, 70],\n     [102, 102, 156], [190, 153, 153], [180, 165, 180], [150, 100, 100],\n     [150, 120, 90], [153, 153, 153], [153, 153, 153], [250, 170, 30],\n     [220, 220, 0], [0, 80, 100]]),\n    \n    np.array([[107, 142, 35], [152, 251, 152]]),\n    np.array([[70, 130, 180]])\n])\n\nconf.one_hot_palette_label = np.array([\n    np.array([[128, 64, 128]]),\n    np.array([[244, 35, 232], [250, 170, 160]]),\n    np.array([[255, 0, 0]]),\n    np.array([[0, 0, 142], [0, 0, 110]]),\n    np.array([[0, 0, 70]]),\n    np.array([[0, 60, 100], [0, 0, 90]]),\n    np.array([[220, 20, 60], [0, 0, 230], [119, 11, 32]]),\n    \n    np.array([[0, 0, 0], [111, 74, 0], [81, 0, 81], [230, 150, 140], [70, 70, 70],\n     [102, 102, 156], [190, 153, 153], [180, 165, 180], [150, 100, 100],\n     [150, 120, 90], [153, 153, 153], [153, 153, 153], [250, 170, 30],\n     [220, 220, 0], [0, 80, 100], [70, 130, 180]]),\n    \n    np.array([[107, 142, 35], [152, 251, 152]]),\n    np.array([[150, 150, 150]])\n])\n\nn_classes_input = len(conf.one_hot_palette_input)\nn_classes_label = len(conf.one_hot_palette_label)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T05:22:08.499142Z","iopub.execute_input":"2021-12-17T05:22:08.499443Z","iopub.status.idle":"2021-12-17T05:22:08.523688Z","shell.execute_reply.started":"2021-12-17T05:22:08.499405Z","shell.execute_reply":"2021-12-17T05:22:08.521508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n# Homography Matrix for dataset\nH = [\n  np.array([[4.651574574230558e-14, 10.192351107009959, -5.36318723862984e-07], [-5.588661045867985e-07, 0.0, 2.3708767903941617], [35.30731833118676, 0.0, -1.7000018578614013]]),                                       # front\n  np.array([[-5.336674306912119e-14, -10.192351107009957, 5.363187220578325e-07], [5.588660952931949e-07, 3.582264351370481e-23, 2.370876772982613], [-35.30731833118661, -2.263156574813233e-15, -0.5999981421386035]]), # rear\n  np.array([[20.38470221401992, 7.562206982469407e-14, -0.28867638384075833], [-3.422067857504854e-23, 2.794330463189411e-07, 2.540225111648729], [2.1619497190382224e-15, -17.65365916559334, -0.4999990710692976]]),    # left\n  np.array([[-20.38470221401991, -4.849709834037436e-15, 0.2886763838407495], [-3.4220679184765114e-23, -2.794330512976549e-07, 2.5402251116487626], [2.161949719038217e-15, 17.653659165593304, -0.5000009289306967]])   # right\n]","metadata":{"execution":{"iopub.status.busy":"2021-12-17T05:22:08.525986Z","iopub.execute_input":"2021-12-17T05:22:08.526503Z","iopub.status.idle":"2021-12-17T05:22:08.535265Z","shell.execute_reply.started":"2021-12-17T05:22:08.526448Z","shell.execute_reply":"2021-12-17T05:22:08.534518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get max_samples_training random training samples\nn_inputs = len(conf.input_training)\nfiles_train_input = [get_files_in_folder(folder) for folder in conf.input_training]\nfiles_train_label = get_files_in_folder(conf.label_training)\n_, idcs = sample_list(files_train_label, n_samples=conf.max_samples_training)\nfiles_train_input = [np.take(f, idcs) for f in files_train_input]\nfiles_train_label = np.take(files_train_label, idcs)\nimage_shape_original_input = load_image(files_train_input[0][0]).shape[0:2]\nimage_shape_original_label = load_image(files_train_label[0]).shape[0:2]\nprint(f\"Found {len(files_train_label)} training samples\")\n\n# get max_samples_validation random validation samples\nfiles_valid_input = [get_files_in_folder(folder) for folder in conf.input_validation]\nfiles_valid_label = get_files_in_folder(conf.label_validation)\n_, idcs = sample_list(files_valid_label, n_samples=conf.max_samples_validation)\nfiles_valid_input = [np.take(f, idcs) for f in files_valid_input]\nfiles_valid_label = np.take(files_valid_label, idcs)\nprint(f\"Found {len(files_valid_label)} validation samples\")","metadata":{"execution":{"iopub.status.busy":"2021-12-17T05:22:08.536296Z","iopub.execute_input":"2021-12-17T05:22:08.536705Z","iopub.status.idle":"2021-12-17T05:22:19.40697Z","shell.execute_reply.started":"2021-12-17T05:22:08.536675Z","shell.execute_reply":"2021-12-17T05:22:19.406119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Loader","metadata":{}},{"cell_type":"code","source":"# build dataset pipeline parsing functions\ndef parse_sample(input_files, label_file):\n    # parse and process input images\n    inputs = []\n    for inp in input_files:\n        inp = load_image_op(inp)\n        inp = resize_image_op(inp, image_shape_original_input, conf.image_shape, interpolation=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n        inp = one_hot_encode_image_op(inp, conf.one_hot_palette_input)\n        inputs.append(inp)\n    inputs = inputs[0] if n_inputs == 1 else tuple(inputs)\n    # parse and process label image\n    label = load_image_op(label_file)\n    label = resize_image_op(label, image_shape_original_label, conf.image_shape, interpolation=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    label = one_hot_encode_image_op(label, conf.one_hot_palette_label)\n    return inputs, label\n\n# build training data pipeline\ndataTrain = tf.data.Dataset.from_tensor_slices((tuple(files_train_input), files_train_label))\ndataTrain = dataTrain.shuffle(buffer_size=conf.max_samples_training, reshuffle_each_iteration=True)\ndataTrain = dataTrain.map(parse_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE)\ndataTrain = dataTrain.batch(conf.batch_size, drop_remainder=True)\ndataTrain = dataTrain.repeat(conf.epochs)\ndataTrain = dataTrain.prefetch(1)\nprint(\"Built data pipeline for training\")\n\n# build validation data pipeline\ndataValid = tf.data.Dataset.from_tensor_slices((tuple(files_valid_input), files_valid_label))\ndataValid = dataValid.map(parse_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE)\ndataValid = dataValid.batch(1)\ndataValid = dataValid.repeat(conf.epochs)\ndataValid = dataValid.prefetch(1)\nprint(\"Built data pipeline for validation\")","metadata":{"execution":{"iopub.status.busy":"2021-12-17T05:22:19.40835Z","iopub.execute_input":"2021-12-17T05:22:19.408574Z","iopub.status.idle":"2021-12-17T05:22:21.325233Z","shell.execute_reply.started":"2021-12-17T05:22:19.408546Z","shell.execute_reply":"2021-12-17T05:22:21.324604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Network Architecture","metadata":{}},{"cell_type":"markdown","source":"## Spatial Transformer","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Layer, Input, Activation, MaxPooling2D, Flatten, Conv2D, Dense, Multiply\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import backend as K\n\n\ndef K_meshgrid(x, y):\n    return tf.meshgrid(x, y)\n\n\ndef K_linspace(start, stop, num):\n    return tf.linspace(start, stop, num)\n\n\nclass BilinearInterpolation(Layer):\n    \"\"\"Performs bilinear interpolation as a keras layer\n    References\n    ----------\n    [1]  Spatial Transformer Networks, Max Jaderberg, et al.\n    [2]  https://github.com/skaae/transformer_network\n    [3]  https://github.com/EderSantana/seya\n    \"\"\"\n\n    def __init__(self, output_size, **kwargs):\n        self.output_size = output_size\n        super(BilinearInterpolation, self).__init__(**kwargs)\n\n    def get_config(self):\n        return {\n            'output_size': self.output_size,\n        }\n\n    def compute_output_shape(self, input_shapes):\n        height, width = self.output_size\n        num_channels = input_shapes[0][-1]\n        return (None, height, width, num_channels)\n\n    def call(self, tensors, mask=None):\n        X, transformation = tensors\n        output = self._transform(X, transformation, self.output_size)\n        return output\n\n    def _interpolate(self, image, sampled_grids, output_size):\n\n        batch_size = K.shape(image)[0]\n        height = K.shape(image)[1]\n        width = K.shape(image)[2]\n        num_channels = K.shape(image)[3]\n\n        x = K.cast(K.flatten(sampled_grids[:, 0:1, :]), dtype='float32')\n        y = K.cast(K.flatten(sampled_grids[:, 1:2, :]), dtype='float32')\n\n        x = .5 * (x + 1.0) * K.cast(width, dtype='float32')\n        y = .5 * (y + 1.0) * K.cast(height, dtype='float32')\n\n        x0 = K.cast(x, 'int32')\n        x1 = x0 + 1\n        y0 = K.cast(y, 'int32')\n        y1 = y0 + 1\n\n        max_x = int(K.int_shape(image)[2] - 1)\n        max_y = int(K.int_shape(image)[1] - 1)\n\n        x0 = K.clip(x0, 0, max_x)\n        x1 = K.clip(x1, 0, max_x)\n        y0 = K.clip(y0, 0, max_y)\n        y1 = K.clip(y1, 0, max_y)\n\n        pixels_batch = K.arange(0, batch_size) * (height * width)\n        pixels_batch = K.expand_dims(pixels_batch, axis=-1)\n        flat_output_size = output_size[0] * output_size[1]\n        base = K.repeat_elements(pixels_batch, flat_output_size, axis=1)\n        base = K.flatten(base)\n\n        # base_y0 = base + (y0 * width)\n        base_y0 = y0 * width\n        base_y0 = base + base_y0\n        # base_y1 = base + (y1 * width)\n        base_y1 = y1 * width\n        base_y1 = base_y1 + base\n\n        indices_a = base_y0 + x0\n        indices_b = base_y1 + x0\n        indices_c = base_y0 + x1\n        indices_d = base_y1 + x1\n\n        flat_image = K.reshape(image, shape=(-1, num_channels))\n        flat_image = K.cast(flat_image, dtype='float32')\n        pixel_values_a = K.gather(flat_image, indices_a)\n        pixel_values_b = K.gather(flat_image, indices_b)\n        pixel_values_c = K.gather(flat_image, indices_c)\n        pixel_values_d = K.gather(flat_image, indices_d)\n\n        x0 = K.cast(x0, 'float32')\n        x1 = K.cast(x1, 'float32')\n        y0 = K.cast(y0, 'float32')\n        y1 = K.cast(y1, 'float32')\n\n        area_a = K.expand_dims(((x1 - x) * (y1 - y)), 1)\n        area_b = K.expand_dims(((x1 - x) * (y - y0)), 1)\n        area_c = K.expand_dims(((x - x0) * (y1 - y)), 1)\n        area_d = K.expand_dims(((x - x0) * (y - y0)), 1)\n\n        values_a = area_a * pixel_values_a\n        values_b = area_b * pixel_values_b\n        values_c = area_c * pixel_values_c\n        values_d = area_d * pixel_values_d\n        return values_a + values_b + values_c + values_d\n\n    def _make_regular_grids(self, batch_size, height, width):\n        # making a single regular grid\n        x_linspace = K_linspace(-1., 1., width)\n        y_linspace = K_linspace(-1., 1., height)\n        x_coordinates, y_coordinates = K_meshgrid(x_linspace, y_linspace)\n        x_coordinates = K.flatten(x_coordinates)\n        y_coordinates = K.flatten(y_coordinates)\n        ones = K.ones_like(x_coordinates)\n        grid = K.concatenate([x_coordinates, y_coordinates, ones], 0)\n\n        # repeating grids for each batch\n        grid = K.flatten(grid)\n        grids = K.tile(grid, K.stack([batch_size]))\n        return K.reshape(grids, (batch_size, 3, height * width))\n\n    def _transform(self, X, transformation, output_size):\n        \n        batch_size, num_channels = K.shape(X)[0], X.shape[3]\n\n        transformation = K.reshape(transformation, shape=(batch_size, 3, 3))\n\n        # create regular grid (-1,1)x(-1,1)\n        regular_grids = self._make_regular_grids(batch_size, *output_size)\n\n        # transform regular grid\n        sampled_grids = K.batch_dot(transformation, regular_grids)\n\n        # homogeneous coords: divide by 3rd component w\n        w = tf.math.reciprocal(sampled_grids[:, 2, :])\n        w = tf.reshape(w, (batch_size, 1, w.shape[-1]))\n        w = tf.tile(w, [1, 2, 1])\n        sampled_grids = Multiply()([sampled_grids[:, 0:2, :], w])\n\n        # interpolate image\n        interpolated_image = self._interpolate(X, sampled_grids, output_size)\n        new_shape = (batch_size, output_size[0], output_size[1], num_channels)\n        interpolated_image = K.reshape(interpolated_image, new_shape)\n\n        return interpolated_image\n\n\ndef STN_example_model(input_shape=(60, 60, 1), sampling_size=(30, 30), num_classes=10):\n\n    image = Input(shape=input_shape)\n    locnet = MaxPooling2D(pool_size=(2, 2))(image)\n    locnet = Conv2D(20, (5, 5))(locnet)\n    locnet = MaxPooling2D(pool_size=(2, 2))(locnet)\n    locnet = Conv2D(20, (5, 5))(locnet)\n    locnet = Flatten()(locnet)\n    locnet = Dense(50)(locnet)\n    locnet = Activation('relu')(locnet)\n    def get_initial_weights(output_size):\n        b = np.zeros((2, 3), dtype='float32')\n        b[0, 0] = 1\n        b[1, 1] = 1\n        W = np.zeros((output_size, 6), dtype='float32')\n        weights = [W, b.flatten()]\n        return weights\n    weights = get_initial_weights(50)\n    locnet = Dense(6, weights=weights)(locnet)\n    x = BilinearInterpolation(sampling_size)([image, locnet])\n    x = Conv2D(32, (3, 3), padding='same')(x)\n    x = Activation('relu')(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n    x = Conv2D(32, (3, 3))(x)\n    x = Activation('relu')(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n    x = Flatten()(x)\n    x = Dense(256)(x)\n    x = Activation('relu')(x)\n    x = Dense(num_classes)(x)\n    x = Activation('softmax')(x)\n    return Model(inputs=image, outputs=x)\n\n\ndef _spatial_transformer(input, output_shape, theta_init=np.eye(3), theta_const=False, loc_downsample=3, dense_units=20, filters=16, kernel_size=(3,3), activation=tf.nn.relu, dense_reg=0.0):\n\n    theta_init = theta_init.flatten().astype(np.float32)\n\n    if not theta_const:\n\n        t = input\n\n        # initialize transform to identity\n        init_weights = [np.zeros((dense_units, 9), dtype=np.float32), theta_init]\n\n        # localization network\n        for d in range(loc_downsample):\n            t = Conv2D(filters=filters, kernel_size=kernel_size, padding=\"same\", activation=activation)(t)\n            t = MaxPooling2D(pool_size=(2,2), padding=\"same\")(t)\n        t = Flatten()(t)\n        t = Dense(dense_units)(t)\n\n        k_reg = tf.keras.regularizers.l2(dense_reg) if dense_reg > 0 else None\n        b_reg = tf.keras.regularizers.l2(dense_reg) if dense_reg > 0 else None\n        theta = Dense(9, weights=init_weights, kernel_regularizer=k_reg, bias_regularizer=b_reg)(t) # transformation parameters\n\n    else:\n\n        theta = tf.tile(theta_init, tf.shape(input)[0:1])\n\n    # transform feature map\n    output = BilinearInterpolation(output_shape)([input, theta])\n\n    return output\n\n\ndef SpatialTransformer(input_shape, output_shape, theta_init=np.eye(3), theta_const=False, loc_downsample=3, dense_units=20, filters=16, kernel_size=(3,3), activation=tf.nn.relu, dense_reg=0.0):\n\n    input = Input(input_shape)\n    output = _spatial_transformer(input, output_shape[0:2], theta_init, theta_const, loc_downsample, dense_units, filters, kernel_size, activation, dense_reg)\n\n    return Model(input, output)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T05:22:21.326398Z","iopub.execute_input":"2021-12-17T05:22:21.326751Z","iopub.status.idle":"2021-12-17T05:22:21.366163Z","shell.execute_reply.started":"2021-12-17T05:22:21.32672Z","shell.execute_reply":"2021-12-17T05:22:21.365478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## UNetXST","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Dense, Conv2D, Conv2DTranspose, MaxPooling2D\nfrom tensorflow.keras.layers import BatchNormalization, Dropout\nfrom tensorflow.keras.layers import Activation\nfrom tensorflow.keras.layers import Concatenate\n\n\ndef encoder(input, udepth=3, filters1=8, kernel_size=(3,3), activation=tf.nn.relu, batch_norm=True, dropout=0.1):\n\n    t = input\n    encoder_layers = udepth * [None]\n\n    # common parameters\n    pool_size = (2,2)\n    padding = \"same\"\n\n    # layer creation with successive pooling\n    for d in range(udepth):\n        filters = (2**d) * filters1\n        t = Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation=activation)(t)\n        t = BatchNormalization()(t) if batch_norm else t\n        t = Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation=activation)(t)\n        t = encoder_layers[d] = BatchNormalization()(t) if batch_norm else t\n        if d < (udepth - 1):\n            t = MaxPooling2D(pool_size=pool_size, padding=padding)(t)\n            t = Dropout(rate=dropout)(t) if dropout > 0 else t\n\n    return encoder_layers\n\n\ndef joiner(list_of_encoder_layers, thetas, filters1=8, kernel_size=(3,3), activation=tf.nn.relu, batch_norm=True, double_skip_connection=False):\n\n    n_inputs = len(list_of_encoder_layers)\n    udepth = len(list_of_encoder_layers[0])\n    encoder_layers = udepth * [None]\n\n    for d in range(udepth):\n        filters = (2**d) * filters1\n        shape = list_of_encoder_layers[0][d].shape[1:]\n\n        warped_maps = []\n        for i in range(n_inputs): # use Spatial Transformer with constant homography transformation before concatenating\n            # Problem w/ trainable theta: regularization necessary, huge loss, always went to loss=nan\n            t = SpatialTransformer(shape, shape, theta_init=thetas[i], theta_const=True)(list_of_encoder_layers[i][d])\n            warped_maps.append(t)\n        t = Concatenate()(warped_maps) if n_inputs > 1 else warped_maps[0]\n        t = Conv2D(filters=filters, kernel_size=kernel_size, padding=\"same\", activation=activation)(t)\n        t = BatchNormalization()(t) if batch_norm else t\n        t = Conv2D(filters=filters, kernel_size=kernel_size, padding=\"same\", activation=activation)(t)\n        t = warped = BatchNormalization()(t) if batch_norm else t\n\n        if not double_skip_connection:\n\n            t = encoder_layers[d] = warped\n\n        else:\n\n            nonwarped_maps = []\n            for i in range(n_inputs): # also concat non-warped maps\n                t = list_of_encoder_layers[i][d]\n                nonwarped_maps.append(t)\n            t = Concatenate()(nonwarped_maps) if n_inputs > 1 else nonwarped_maps[0]\n            t = Conv2D(filters=filters, kernel_size=kernel_size, padding=\"same\", activation=activation)(t)\n            t = BatchNormalization()(t) if batch_norm else t\n            t = Conv2D(filters=filters, kernel_size=kernel_size, padding=\"same\", activation=activation)(t)\n            t = nonwarped = BatchNormalization()(t) if batch_norm else t\n\n            # concat both\n            t = Concatenate()([warped, nonwarped])\n            t = Conv2D(filters=filters, kernel_size=kernel_size, padding=\"same\", activation=activation)(t)\n            t = BatchNormalization()(t) if batch_norm else t\n            t = Conv2D(filters=filters, kernel_size=kernel_size, padding=\"same\", activation=activation)(t)\n            t = encoder_layers[d] = BatchNormalization()(t) if batch_norm else t\n\n    return encoder_layers\n\n\ndef decoder(encoder_layers, udepth=3, filters1=8, kernel_size=(3,3), activation=tf.nn.relu, batch_norm=True, dropout=0.1):\n\n    # start at lowest encoder layer\n    t = encoder_layers[udepth-1]\n\n    # common parameters\n    strides = (2,2)\n    padding = \"same\"\n\n    # layer expansion symmetric to encoder\n    for d in reversed(range(udepth-1)):\n        filters = (2**d) * filters1\n        t = Conv2DTranspose(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding)(t)\n        t = Concatenate()([encoder_layers[d], t])\n        t = Dropout(rate=dropout)(t) if dropout > 0 else t\n        t = Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation=activation)(t)\n        t = BatchNormalization()(t) if batch_norm else t\n        t = Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation=activation)(t)\n        t = BatchNormalization()(t) if batch_norm else t\n\n    return t\n\n\ndef get_network(input_shape, n_output_channels, n_inputs, thetas, \n                udepth = 5, \n                filters1 = 16, \n                kernel_size = (3,3), \n                activation = tf.nn.relu, \n                batch_norm = True, \n                dropout = 0.1,\n                double_skip_connection = False):\n\n    # build inputs\n    inputs = [Input(input_shape) for i in range(n_inputs)]\n\n    # encode all inputs separately\n    list_of_encoder_layers = []\n    for i in inputs:\n        encoder_layers = encoder(i, udepth, filters1, kernel_size, activation, batch_norm, dropout)\n        list_of_encoder_layers.append(encoder_layers)\n\n    # fuse encodings of all inputs at all layers\n    encoder_layers = joiner(list_of_encoder_layers, thetas, filters1, kernel_size, activation, batch_norm, double_skip_connection)\n\n    # decode from bottom to top layer\n    reconstruction = decoder(encoder_layers, udepth, filters1, kernel_size, activation, batch_norm, dropout)\n\n    # build final prediction layer\n    prediction = Conv2D(filters=n_output_channels, kernel_size=kernel_size, padding=\"same\", activation=activation)(reconstruction)\n    prediction = Activation(\"softmax\")(prediction)\n\n    return Model(inputs, prediction)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T05:22:21.367807Z","iopub.execute_input":"2021-12-17T05:22:21.368404Z","iopub.status.idle":"2021-12-17T05:22:21.39231Z","shell.execute_reply.started":"2021-12-17T05:22:21.368368Z","shell.execute_reply":"2021-12-17T05:22:21.391683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build Model","metadata":{}},{"cell_type":"code","source":"model = get_network((conf.image_shape[0], conf.image_shape[1], n_classes_input), n_classes_label, n_inputs=n_inputs, thetas=H)\noptimizer = tf.keras.optimizers.Adam(learning_rate=conf.learning_rate)\nloss = weighted_categorical_crossentropy(conf.loss_weights)\n\nmetrics = [tf.keras.metrics.CategoricalAccuracy(), MeanIoUWithOneHotLabels(num_classes=n_classes_label)]\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\nprint(f\"Compiled model\")","metadata":{"execution":{"iopub.status.busy":"2021-12-17T05:22:21.39388Z","iopub.execute_input":"2021-12-17T05:22:21.394371Z","iopub.status.idle":"2021-12-17T05:22:34.552344Z","shell.execute_reply.started":"2021-12-17T05:22:21.394331Z","shell.execute_reply":"2021-12-17T05:22:34.55134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"from datetime import datetime\nnow_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")","metadata":{"execution":{"iopub.status.busy":"2021-12-17T05:22:34.554523Z","iopub.execute_input":"2021-12-17T05:22:34.554751Z","iopub.status.idle":"2021-12-17T05:22:34.558966Z","shell.execute_reply.started":"2021-12-17T05:22:34.554722Z","shell.execute_reply":"2021-12-17T05:22:34.557991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create output directories\nmodel_output_dir = os.path.join(conf.output_dir, now_time)\ntensorboard_dir = os.path.join(model_output_dir, \"TensorBoard\")\ncheckpoint_dir  = os.path.join(model_output_dir, \"Checkpoints\")\nif not os.path.exists(tensorboard_dir):\n    os.makedirs(tensorboard_dir)\nif not os.path.exists(checkpoint_dir):\n    os.makedirs(checkpoint_dir)\n\n\n# create callbacks to be called after each epoch\nn_batches_train = len(files_train_label) // conf.batch_size\nn_batches_valid = len(files_valid_label)\ntensorboard_cb      = tf.keras.callbacks.TensorBoard(tensorboard_dir, update_freq=\"epoch\", profile_batch=0)\ncheckpoint_cb       = tf.keras.callbacks.ModelCheckpoint(os.path.join(checkpoint_dir, \"e{epoch:03d}_weights.hdf5\"), save_freq=n_batches_train*conf.save_interval, save_weights_only=True)\nbest_checkpoint_cb  = tf.keras.callbacks.ModelCheckpoint(os.path.join(checkpoint_dir, \"best_weights.hdf5\"), save_best_only=True, monitor=\"val_mean_io_u_with_one_hot_labels\", mode=\"max\", save_weights_only=True)\nearly_stopping_cb   = tf.keras.callbacks.EarlyStopping(monitor=\"val_mean_io_u_with_one_hot_labels\", mode=\"max\", patience=conf.early_stopping_patience, verbose=1)\ncallbacks = [tensorboard_cb, checkpoint_cb, best_checkpoint_cb, early_stopping_cb]","metadata":{"execution":{"iopub.status.busy":"2021-12-17T05:22:34.559814Z","iopub.execute_input":"2021-12-17T05:22:34.56001Z","iopub.status.idle":"2021-12-17T05:22:34.572131Z","shell.execute_reply.started":"2021-12-17T05:22:34.559987Z","shell.execute_reply":"2021-12-17T05:22:34.571529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Retraining for better performance\n# These weights are from a previous training\nmodel.load_weights('../input/temporary/best_weights.hdf5')\nprint(f\"Reloaded weights\")","metadata":{"execution":{"iopub.status.busy":"2021-12-17T05:22:34.573511Z","iopub.execute_input":"2021-12-17T05:22:34.573707Z","iopub.status.idle":"2021-12-17T05:22:36.011329Z","shell.execute_reply.started":"2021-12-17T05:22:34.573683Z","shell.execute_reply":"2021-12-17T05:22:36.010281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Starting training...\")\nmodel.fit(dataTrain,\n          epochs=conf.epochs, steps_per_epoch=n_batches_train,\n          validation_data=dataValid, validation_freq=1, validation_steps=n_batches_valid,\n          callbacks=callbacks)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T05:23:37.442042Z","iopub.execute_input":"2021-12-17T05:23:37.442639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"conf.model_weights = os.path.join(checkpoint_dir, 'best_weights.hdf5') \nconf.prediction_dir = os.path.join(model_output_dir, 'Predictions')\nif not os.path.exists(conf.prediction_dir):\n    os.makedirs(conf.prediction_dir)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T05:22:36.012508Z","iopub.execute_input":"2021-12-17T05:22:36.012758Z","iopub.status.idle":"2021-12-17T05:22:36.016897Z","shell.execute_reply.started":"2021-12-17T05:22:36.01273Z","shell.execute_reply":"2021-12-17T05:22:36.016364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get max_samples_testing samples\nfiles_input = [get_files_in_folder(folder) for folder in conf.input_testing]\nfiles_label = get_files_in_folder(conf.label_testing)\n_, idcs = sample_list(files_input[0], n_samples=conf.max_samples_testing)\nfiles_input = [np.take(f, idcs) for f in files_input]\nfiles_label = np.take(files_label, idcs)\n\nn_inputs = len(conf.input_testing)\nn_samples = len(files_input[0])\nimage_shape_original = load_image(files_input[0][0]).shape[0:2]\nimage_shape_original_label = load_image(files_label[0]).shape[0:2]\n\nprint(f\"Found {n_samples} samples\")\n\n# build data parsing function\ndef parse_sample(input_files):\n    # parse and process input images\n    inputs = []\n    for inp in input_files:\n        inp = load_image_op(inp)\n        inp = resize_image_op(inp, image_shape_original, conf.image_shape, interpolation=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n        inp = one_hot_encode_image_op(inp, conf.one_hot_palette_input)\n        inputs.append(inp)\n    inputs = inputs[0] if n_inputs == 1 else tuple(inputs)\n    return inputs","metadata":{"execution":{"iopub.status.busy":"2021-12-17T05:22:36.01797Z","iopub.execute_input":"2021-12-17T05:22:36.018394Z","iopub.status.idle":"2021-12-17T05:22:36.118057Z","shell.execute_reply.started":"2021-12-17T05:22:36.018364Z","shell.execute_reply":"2021-12-17T05:22:36.117139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.load_weights('../input/temporary/best_weights.hdf5')\n# print(f\"Reloaded model from {conf.model_weights}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-17T05:22:36.11921Z","iopub.execute_input":"2021-12-17T05:22:36.119452Z","iopub.status.idle":"2021-12-17T05:22:36.123246Z","shell.execute_reply.started":"2021-12-17T05:22:36.119425Z","shell.execute_reply":"2021-12-17T05:22:36.122231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\ntitles = [\"Front\", \"Rear\", \"Left\", \"Right\", \"BEV\", \"Actual BEV\"]\n\n# run predictions\nprint(f\"Running predictions and writing to {conf.prediction_dir} ...\")\nfor k in random.sample(range(n_samples), 10):\n\n    input_files = [files_input[i][k] for i in range(n_inputs)]\n    label_file = files_label[k]\n\n    # load sample\n    inputs = parse_sample(input_files)\n\n    # add batch dim\n    if n_inputs > 1:\n        inputs = [np.expand_dims(i, axis=0) for i in inputs]\n    else:\n        inputs = np.expand_dims(inputs, axis=0)\n\n    # run prediction\n    prediction = model.predict(inputs).squeeze()\n\n    # convert to output image\n    prediction = one_hot_decode_image(prediction, conf.one_hot_palette_label)\n    \n    plt.figure(figsize=(25,7))\n    plt.tight_layout()\n    for i in range(n_inputs + 2):\n        ax = plt.subplot(2, 4, i + 1)\n        \n        if i == n_inputs:\n            img = prediction\n        elif i == n_inputs + 1:\n            img = load_image(label_file)\n        else:\n            img = load_image(input_files[i])\n            \n        plt.axis('off')\n        plt.imshow(img)\n        \n    plt.show()\n            \n    # write to disk\n    output_file = os.path.join(conf.prediction_dir, os.path.basename(files_input[0][k]))\n    cv2.imwrite(output_file, cv2.cvtColor(prediction, cv2.COLOR_RGB2BGR))","metadata":{"execution":{"iopub.status.busy":"2021-12-17T05:22:36.124404Z","iopub.execute_input":"2021-12-17T05:22:36.124601Z","iopub.status.idle":"2021-12-17T05:23:11.188666Z","shell.execute_reply.started":"2021-12-17T05:22:36.124576Z","shell.execute_reply":"2021-12-17T05:23:11.187571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}