{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":2885120,"sourceType":"datasetVersion","datasetId":1767359}],"dockerImageVersionId":30153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# MTAN - Multi Task Attention Network\n\nPaper: https://arxiv.org/abs/1803.10704\n\nGithub: https://github.com/lorenmt/mtan","metadata":{}},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data.dataset import Dataset\nimport torch.utils.data.sampler as sampler\n\nimport os\nimport fnmatch\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-03-23T23:58:37.399990Z","iopub.execute_input":"2024-03-23T23:58:37.400197Z","iopub.status.idle":"2024-03-23T23:58:45.916577Z","shell.execute_reply.started":"2024-03-23T23:58:37.400134Z","shell.execute_reply":"2024-03-23T23:58:45.915727Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Loader","metadata":{}},{"cell_type":"code","source":"class CityScapes(Dataset):\n    def __init__(self, root, train=True):\n        self.train = train\n        self.root = os.path.expanduser(root)\n\n        # read the data file\n        if train:\n            self.data_path = root + '/train'\n        else:\n            self.data_path = root + '/val'\n\n        # calculate data length\n        self.data_len = len(fnmatch.filter(os.listdir(self.data_path + '/image'), '*.npy'))\n\n    def __getitem__(self, index):\n        # load data from the pre-processed npy files\n        image = torch.from_numpy(np.moveaxis(np.load(self.data_path + '/image/{:d}.npy'.format(index)), -1, 0))\n        semantic = torch.from_numpy(np.load(self.data_path + '/label/{:d}.npy'.format(index)))\n        depth = torch.from_numpy(np.moveaxis(np.load(self.data_path + '/depth/{:d}.npy'.format(index)), -1, 0))\n        \n        return image.float(), semantic.float(), depth.float()\n\n    def __len__(self):\n        return self.data_len","metadata":{"execution":{"iopub.status.busy":"2024-03-23T23:58:45.918110Z","iopub.execute_input":"2024-03-23T23:58:45.918373Z","iopub.status.idle":"2024-03-23T23:58:45.925975Z","shell.execute_reply.started":"2024-03-23T23:58:45.918331Z","shell.execute_reply":"2024-03-23T23:58:45.925403Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Utility Functions\n\nDefine Task Metrics, Loss Functions and Model Trainer","metadata":{}},{"cell_type":"code","source":"def model_fit(x_pred, x_output, task_type):\n    device = x_pred.device\n\n    # binary mark to mask out undefined pixel space\n    binary_mask = (torch.sum(x_output, dim=1) != 0).float().unsqueeze(1).to(device)\n\n    if task_type == 'semantic':\n        # semantic loss: depth-wise cross entropy\n        loss = F.nll_loss(x_pred, x_output, ignore_index=-1)\n\n    if task_type == 'depth':\n        # depth loss: l1 norm\n        loss = torch.sum(torch.abs(x_pred - x_output) * binary_mask) / torch.nonzero(binary_mask, as_tuple=False).size(0)\n\n    return loss\n\n\n# mIoU and Acc. formula: accumulate every pixel and average across all pixels in all images\nclass ConfMatrix(object):\n    def __init__(self, num_classes):\n        self.num_classes = num_classes\n        self.mat = None\n\n    def update(self, pred, target):\n        n = self.num_classes\n        if self.mat is None:\n            self.mat = torch.zeros((n, n), dtype=torch.int64, device=pred.device)\n        with torch.no_grad():\n            k = (target >= 0) & (target < n)\n            inds = n * target[k].to(torch.int64) + pred[k]\n            self.mat += torch.bincount(inds, minlength=n ** 2).reshape(n, n)\n\n    def get_metrics(self):\n        h = self.mat.float()\n        acc = torch.diag(h).sum() / h.sum()\n        iu = torch.diag(h) / (h.sum(1) + h.sum(0) - torch.diag(h))\n        return torch.mean(iu), acc\n    \n\ndef depth_error(x_pred, x_output):\n    device = x_pred.device\n    binary_mask = (torch.sum(x_output, dim=1) != 0).unsqueeze(1).to(device)\n    x_pred_true = x_pred.masked_select(binary_mask)\n    x_output_true = x_output.masked_select(binary_mask)\n    abs_err = torch.abs(x_pred_true - x_output_true)\n    rel_err = torch.abs(x_pred_true - x_output_true) / x_output_true\n    return (torch.sum(abs_err) / torch.nonzero(binary_mask, as_tuple=False).size(0)).item(), \\\n           (torch.sum(rel_err) / torch.nonzero(binary_mask, as_tuple=False).size(0)).item()\n\n\ndef multi_task_trainer(train_loader, test_loader, multi_task_model, device, optimizer, scheduler, config, total_epoch=200):\n    train_batch = len(train_loader)\n    test_batch = len(test_loader)\n    \n    T = config['temp']\n    avg_cost = np.zeros([total_epoch, 12], dtype=np.float32)\n    lambda_weight = np.ones([2, total_epoch])\n    \n    for index in range(total_epoch):\n        cost = np.zeros(12, dtype=np.float32)\n\n        # apply Dynamic Weight Average\n        if config['weight'] == 'dwa':\n            if index == 0 or index == 1:\n                lambda_weight[:, index] = 1.0\n            else:\n                w_1 = avg_cost[index - 1, 0] / avg_cost[index - 2, 0]\n                w_2 = avg_cost[index - 1, 3] / avg_cost[index - 2, 3]\n                lambda_weight[0, index] = 2 * np.exp(w_1 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T))\n                lambda_weight[1, index] = 2 * np.exp(w_2 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T))\n\n        # iteration for all batches\n        multi_task_model.train()\n        train_dataset = iter(train_loader)\n        conf_mat = ConfMatrix(multi_task_model.class_nb)\n        for k in range(train_batch):\n            train_data, train_label, train_depth = train_dataset.next()\n            train_data, train_label = train_data.to(device), train_label.long().to(device)\n            train_depth = train_depth.to(device)\n\n            train_pred, logsigma = multi_task_model(train_data)\n\n            optimizer.zero_grad()\n            train_loss = [model_fit(train_pred[0], train_label, 'semantic'),\n                          model_fit(train_pred[1], train_depth, 'depth')]\n\n            if config['weight'] == 'equal' or config['weight'] == 'dwa':\n                loss = sum([lambda_weight[i, index] * train_loss[i] for i in range(2)])\n            else:\n                loss = sum(1 / (2 * torch.exp(logsigma[i])) * train_loss[i] + logsigma[i] / 2 for i in range(2))\n\n            loss.backward()\n            optimizer.step()\n\n            # accumulate label prediction for every pixel in training images\n            conf_mat.update(train_pred[0].argmax(1).flatten(), train_label.flatten())\n\n            cost[0] = train_loss[0].item()\n            cost[3] = train_loss[1].item()\n            cost[4], cost[5] = depth_error(train_pred[1], train_depth)\n            avg_cost[index, :6] += cost[:6] / train_batch\n\n        # compute mIoU and acc\n        avg_cost[index, 1:3] = conf_mat.get_metrics()\n\n        # evaluating test data\n        multi_task_model.eval()\n        conf_mat = ConfMatrix(multi_task_model.class_nb)\n        with torch.no_grad():  # operations inside don't track history\n            test_dataset = iter(test_loader)\n            for k in range(test_batch):\n                test_data, test_label, test_depth = test_dataset.next()\n                test_data, test_label = test_data.to(device), test_label.long().to(device)\n                test_depth = test_depth.to(device)\n\n                test_pred, _ = multi_task_model(test_data)\n                test_loss = [model_fit(test_pred[0], test_label, 'semantic'),\n                             model_fit(test_pred[1], test_depth, 'depth')]\n\n                conf_mat.update(test_pred[0].argmax(1).flatten(), test_label.flatten())\n\n                cost[6] = test_loss[0].item()\n                cost[9] = test_loss[1].item()\n                cost[10], cost[11] = depth_error(test_pred[1], test_depth)\n                avg_cost[index, 6:] += cost[6:] / test_batch\n\n            # compute mIoU and acc\n            avg_cost[index, 7:9] = conf_mat.get_metrics()\n\n        scheduler.step()\n        print('Epoch: {:04d} | TRAIN: {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} ||'\n            'TEST: {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} '\n            .format(index, avg_cost[index, 0], avg_cost[index, 1], avg_cost[index, 2], avg_cost[index, 3],\n                    avg_cost[index, 4], avg_cost[index, 5], avg_cost[index, 6], avg_cost[index, 7], avg_cost[index, 8],\n                    avg_cost[index, 9], avg_cost[index, 10], avg_cost[index, 11]))","metadata":{"execution":{"iopub.status.busy":"2024-03-23T23:58:45.927146Z","iopub.execute_input":"2024-03-23T23:58:45.927345Z","iopub.status.idle":"2024-03-23T23:58:45.951368Z","shell.execute_reply.started":"2024-03-23T23:58:45.927320Z","shell.execute_reply":"2024-03-23T23:58:45.950778Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Network\nDefine the network architecture","metadata":{}},{"cell_type":"code","source":"class SegNet(nn.Module):\n    def __init__(self):\n        super(SegNet, self).__init__()\n        \n        # initialise network parameters\n        filter = [64, 128, 256, 512, 512]\n        self.class_nb = 19\n\n        # define encoder decoder layers\n        self.encoder_block = nn.ModuleList([self.conv_layer([3, filter[0]])])\n        self.decoder_block = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            self.encoder_block.append(self.conv_layer([filter[i], filter[i + 1]]))\n            self.decoder_block.append(self.conv_layer([filter[i + 1], filter[i]]))\n\n        # define convolution layer\n        self.conv_block_enc = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        self.conv_block_dec = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            if i == 0:\n                self.conv_block_enc.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n                self.conv_block_dec.append(self.conv_layer([filter[i], filter[i]]))\n            else:\n                self.conv_block_enc.append(nn.Sequential(self.conv_layer([filter[i + 1], filter[i + 1]]),\n                                                         self.conv_layer([filter[i + 1], filter[i + 1]])))\n                self.conv_block_dec.append(nn.Sequential(self.conv_layer([filter[i], filter[i]]),\n                                                         self.conv_layer([filter[i], filter[i]])))\n\n        # define task attention layers\n        self.encoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])])])\n        self.decoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])])])\n        self.encoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[1]])])\n        self.decoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\n        for j in range(2):\n            if j < 2:\n                self.encoder_att.append(nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])]))\n                self.decoder_att.append(nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])]))\n            for i in range(4):\n                self.encoder_att[j].append(self.att_layer([2 * filter[i + 1], filter[i + 1], filter[i + 1]]))\n                self.decoder_att[j].append(self.att_layer([filter[i + 1] + filter[i], filter[i], filter[i]]))\n\n        for i in range(4):\n            if i < 3:\n                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 2]]))\n                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i]]))\n            else:\n                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n\n        self.pred_task1 = self.conv_layer([filter[0], self.class_nb], pred=True)\n        self.pred_task2 = self.conv_layer([filter[0], 1], pred=True)\n\n        # define pooling and unpooling functions\n        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\n        self.logsigma = nn.Parameter(torch.FloatTensor([-0.5, -0.5, -0.5]))\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n\n    def conv_layer(self, channel, pred=False):\n        if not pred:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(num_features=channel[1]),\n                nn.ReLU(inplace=True),\n            )\n        else:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[0], kernel_size=3, padding=1),\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n            )\n        return conv_block\n\n    def att_layer(self, channel):\n        att_block = nn.Sequential(\n            nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n            nn.BatchNorm2d(channel[1]),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=1, padding=0),\n            nn.BatchNorm2d(channel[2]),\n            nn.Sigmoid(),\n        )\n        return att_block\n\n    def forward(self, x):\n        g_encoder, g_decoder, g_maxpool, g_upsampl, indices = ([0] * 5 for _ in range(5))\n        for i in range(5):\n            g_encoder[i], g_decoder[-i - 1] = ([0] * 2 for _ in range(2))\n\n        # define attention list for tasks\n        atten_encoder, atten_decoder = ([0] * 3 for _ in range(2))\n        for i in range(2):\n            atten_encoder[i], atten_decoder[i] = ([0] * 5 for _ in range(2))\n        for i in range(2):\n            for j in range(5):\n                atten_encoder[i][j], atten_decoder[i][j] = ([0] * 3 for _ in range(2))\n\n        # define global shared network\n        for i in range(5):\n            if i == 0:\n                g_encoder[i][0] = self.encoder_block[i](x)\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n            else:\n                g_encoder[i][0] = self.encoder_block[i](g_maxpool[i - 1])\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\n        for i in range(5):\n            if i == 0:\n                g_upsampl[i] = self.up_sampling(g_maxpool[-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n            else:\n                g_upsampl[i] = self.up_sampling(g_decoder[i - 1][-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\n        # define task dependent attention module\n        for i in range(2):\n            for j in range(5):\n                if j == 0:\n                    atten_encoder[i][j][0] = self.encoder_att[i][j](g_encoder[j][0])\n                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n                else:\n                    atten_encoder[i][j][0] = self.encoder_att[i][j](torch.cat((g_encoder[j][0], atten_encoder[i][j - 1][2]), dim=1))\n                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n\n            for j in range(5):\n                if j == 0:\n                    atten_decoder[i][j][0] = F.interpolate(atten_encoder[i][-1][-1], scale_factor=2, mode='bilinear', align_corners=True)\n                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat((g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n                else:\n                    atten_decoder[i][j][0] = F.interpolate(atten_decoder[i][j - 1][2], scale_factor=2, mode='bilinear', align_corners=True)\n                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat((g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n\n        # define task prediction layers\n        t1_pred = F.log_softmax(self.pred_task1(atten_decoder[0][-1][-1]), dim=1)\n        t2_pred = self.pred_task2(atten_decoder[1][-1][-1])\n\n        return [t1_pred, t2_pred], self.logsigma","metadata":{"execution":{"iopub.status.busy":"2024-03-23T23:58:45.953078Z","iopub.execute_input":"2024-03-23T23:58:45.953419Z","iopub.status.idle":"2024-03-23T23:58:45.993079Z","shell.execute_reply.started":"2024-03-23T23:58:45.953387Z","shell.execute_reply":"2024-03-23T23:58:45.992516Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nconfig = {\n    'temp': 2.0,\n    'weight': 'dwa'\n}","metadata":{"execution":{"iopub.status.busy":"2024-03-23T23:58:45.993855Z","iopub.execute_input":"2024-03-23T23:58:45.994028Z","iopub.status.idle":"2024-03-23T23:58:46.007070Z","shell.execute_reply.started":"2024-03-23T23:58:45.994006Z","shell.execute_reply":"2024-03-23T23:58:46.006474Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Model, Optimizer and Scheduler","metadata":{}},{"cell_type":"code","source":"SegNet_MTAN = SegNet().to(device)\noptimizer = optim.Adam(SegNet_MTAN.parameters(), lr=1e-3)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n\nprint('LOSS FORMAT: SEMANTIC_LOSS MEAN_IOU PIX_ACC | DEPTH_LOSS ABS_ERR REL_ERR <11.25 <22.5')","metadata":{"execution":{"iopub.status.busy":"2024-03-23T23:58:46.007889Z","iopub.execute_input":"2024-03-23T23:58:46.008065Z","iopub.status.idle":"2024-03-23T23:58:46.682475Z","shell.execute_reply.started":"2024-03-23T23:58:46.008043Z","shell.execute_reply":"2024-03-23T23:58:46.681737Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"LOSS FORMAT: SEMANTIC_LOSS MEAN_IOU PIX_ACC | DEPTH_LOSS ABS_ERR REL_ERR <11.25 <22.5\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"dataset_path = '../input/cityscapes-depth-and-segmentation/data'\ntrain_set = CityScapes(root=dataset_path, train=True)\ntest_set = CityScapes(root=dataset_path, train=False)\n\nbatch_size = 8\ntrain_loader = torch.utils.data.DataLoader(\n               dataset=train_set,\n               batch_size=batch_size,\n               shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(\n              dataset=test_set,\n              batch_size=batch_size,\n              shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-23T23:58:46.683445Z","iopub.execute_input":"2024-03-23T23:58:46.683635Z","iopub.status.idle":"2024-03-23T23:58:46.967345Z","shell.execute_reply.started":"2024-03-23T23:58:46.683612Z","shell.execute_reply":"2024-03-23T23:58:46.966574Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"multi_task_trainer(train_loader,\n                   test_loader,\n                   SegNet_MTAN,\n                   device,\n                   optimizer,\n                   scheduler,\n                   config,\n                   100)","metadata":{"execution":{"iopub.status.busy":"2024-03-23T23:58:46.968400Z","iopub.execute_input":"2024-03-23T23:58:46.968878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"VALID_DIR = '../input/cityscapes-depth-and-segmentation/data/val/'\nn_files = 10\nfiles = random.sample(os.listdir(VALID_DIR + 'image'), n_files)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"segmentations = []\ndepths = []\n\nfor file in files:\n    img = torch.from_numpy(np.expand_dims(np.moveaxis(np.load(VALID_DIR + 'image/' + file), -1, 0), axis = 0))\n    img = img.to(device)\n    prediction, _ = SegNet_MTAN(img.float())\n    \n    seg_prediction = np.moveaxis(prediction[0][0].cpu().detach().numpy(), 0, -1).argmax(2)\n    depth_prediction = np.moveaxis(prediction[1][0].cpu().detach().numpy(), 0, -1)\n    \n    segmentations.append(seg_prediction)\n    depths.append(depth_prediction)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Segmentation Labels","metadata":{}},{"cell_type":"code","source":"for file, seg_pred in zip(files, segmentations):\n    seg_label = np.load(VALID_DIR + 'label/' + file)\n    \n    plt.figure(figsize = (20,10))\n    plt.subplot(1, 2, 1)\n    plt.imshow(seg_label)\n\n    plt.subplot(1, 2, 2)\n    plt.imshow(seg_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for file, seg_pred in zip(files, segmentations):\n    img = np.load(VALID_DIR + 'image/' + file)\n    \n    plt.figure(figsize = (10,10))\n    plt.imshow(img)\n    plt.imshow(seg_pred, alpha = 0.7)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Depth Maps","metadata":{}},{"cell_type":"code","source":"for file, depth_pred in zip(files, depths):\n    depth_label = np.load(VALID_DIR + 'depth/' + file)\n    \n    plt.figure(figsize = (20,10))\n    plt.subplot(1, 2, 1)\n    plt.imshow(depth_label)\n\n    plt.subplot(1, 2, 2)\n    plt.imshow(depth_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for file, depth_pred in zip(files, depths):\n    img = np.load(VALID_DIR + 'image/' + file)\n    \n    plt.figure(figsize = (10,10))\n    plt.imshow(img)\n    plt.imshow(depth_pred, alpha = 0.7)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}